{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to wai-annotations wai-annotations is a Python library for converting annotation datasets between various annotation formats of data domains such as images and audio. With its command-line tools, you can, e.g., create conversion pipelines or split dataset batches into subsets. The library also offers plugins for reading/writing videos, preprocessing data, for generating statistics and visualizations, as well as incorporating predictions made via a Redis backend (e.g., using deep learning models). Installation - how to install the library Usage - running the library from the command-line Examples - common use-cases Docker - using wai.annotations via pre-built docker images Architecture overview - description of the internals Plugin Guide - adding new domains/formats/conversions Domains - supported domains Plugins - available plugins Glossary - explanations of terms","title":"Home"},{"location":"#welcome-to-wai-annotations","text":"wai-annotations is a Python library for converting annotation datasets between various annotation formats of data domains such as images and audio. With its command-line tools, you can, e.g., create conversion pipelines or split dataset batches into subsets. The library also offers plugins for reading/writing videos, preprocessing data, for generating statistics and visualizations, as well as incorporating predictions made via a Redis backend (e.g., using deep learning models). Installation - how to install the library Usage - running the library from the command-line Examples - common use-cases Docker - using wai.annotations via pre-built docker images Architecture overview - description of the internals Plugin Guide - adding new domains/formats/conversions Domains - supported domains Plugins - available plugins Glossary - explanations of terms","title":"Welcome to wai-annotations"},{"location":"architecture_overview/","text":"Architecture Overview The wai-annotations architecture is made of a few layers: the stream layer the component layer the stage layer Stream Layer The stream layer is the lowest-level stream-processing layer of the wai-annotations architecture. See here for a detailed description of the stream layer. Component Layer The component layer is an extension to the stream layer, adding logging support and command-line parsing to each of the source, processor and sink streaming base-classes. The resulting base-classes are SourceComponent, ProcessorComponent, and SinkComponent. To use logging for a component, simply call the get_class_logger method from a component class, or access the logger property from a component instance. See here for a description of how to add command-line options to a component. Stage Layer The stage layer groups partial-pipelines of components into conceptual \"stages\", where each stage represents an operation at the dataset level in the pipeline. For example, the input stage of reading a dataset in some format might involve a source component which parses valid filenames from the command-line, and passes them to a processor-component which reads those files and passes on the actual file data. Stages are specified by specifier classes (SourceStageSpecifier, ProcessorStageSpecifier, SinkStageSpecifier), which provide the components used to create the stage, and also provide a description of the A.I. domains that the stage can work with. For source/sink stages, a single domain is provided which defines the domain for which the component can produce/consume datasets. For processor stages, a domain transfer-function is defined on the specifier, which determines the output domain for a given input domain (or declares the input domain unusable with the processor stage). wai-annotations uses the domain specifications to check that a series of stages is specified that will be able to pass data from one stage to the next without error. For information on how to create new processing stages, see here","title":"Architecture overview"},{"location":"architecture_overview/#architecture-overview","text":"The wai-annotations architecture is made of a few layers: the stream layer the component layer the stage layer","title":"Architecture Overview"},{"location":"architecture_overview/#stream-layer","text":"The stream layer is the lowest-level stream-processing layer of the wai-annotations architecture. See here for a detailed description of the stream layer.","title":"Stream Layer"},{"location":"architecture_overview/#component-layer","text":"The component layer is an extension to the stream layer, adding logging support and command-line parsing to each of the source, processor and sink streaming base-classes. The resulting base-classes are SourceComponent, ProcessorComponent, and SinkComponent. To use logging for a component, simply call the get_class_logger method from a component class, or access the logger property from a component instance. See here for a description of how to add command-line options to a component.","title":"Component Layer"},{"location":"architecture_overview/#stage-layer","text":"The stage layer groups partial-pipelines of components into conceptual \"stages\", where each stage represents an operation at the dataset level in the pipeline. For example, the input stage of reading a dataset in some format might involve a source component which parses valid filenames from the command-line, and passes them to a processor-component which reads those files and passes on the actual file data. Stages are specified by specifier classes (SourceStageSpecifier, ProcessorStageSpecifier, SinkStageSpecifier), which provide the components used to create the stage, and also provide a description of the A.I. domains that the stage can work with. For source/sink stages, a single domain is provided which defines the domain for which the component can produce/consume datasets. For processor stages, a domain transfer-function is defined on the specifier, which determines the output domain for a given input domain (or declares the input domain unusable with the processor stage). wai-annotations uses the domain specifications to check that a series of stages is specified that will be able to pass data from one stage to the next without error. For information on how to create new processing stages, see here","title":"Stage Layer"},{"location":"cli/","text":"CLI Support in wai-annotations Command-line options are added to wai-annotations at the component level via the use of Option descriptor classes. This allows for adding command-line options in a declarative style, specifying information about the nature and type of the options, while letting wai-annotations automatically perform the work of getting information from the user and presenting it to the components at run-time. To add an option to a component, create a class-level variable and assign an Option instance to it (see below for a description of the available Option types). To read the value of an option as specified by the user, simply access the class-level variable through the component instance e.g. self.my_option as if it were a property. N.B. Although options are defined at the component level, wai-annotations aggregates them at the stage level. Therefore, care must be taken not to use the same flag/s for multiple options, not only within a component (including its ancestor classes), but also between components that are shared by a stage. Types of Options ClassOption An option for selecting a Python class. Is initialised with a class-registry of the available choices the user can select from. The user provides the name of a class in the registry and the class itself is returned. CountOption Option which simply returns the number of times its flag was supplied to the command line. This is useful, for example, for verbosity values, where increasing the number of flags increases the logging verbosity (e.g. -vvv ). An optional translation table can be provided which translates the actual value to some mapped value. FlagOption A simple flag, which evaluates to True when provided on the command-line, or False if not (these semantics can be inverted). TypedOption The most versatile option, it takes a type to use to parse the command-line string/s provide by the user. Can return a single-value of that type or a list of values of that type if set to take multiple arguments. Example This example creates a processor component which takes a schedule of integers from the command line, and multiplies incoming integers by the next integer in the schedule. # For typing the schedule from typing import List # The type of component we are making from wai.annotations.core.component import ProcessorComponent # or SourceComponent, or SinkComponent # The type of option we want to add to our component from wai.common.cli.options import TypedOption # or ClassOption, or CountOption, or FlagOption # We need some process-state to track where we are in the schedule. We output results as we get them, so no # buffering occurs and therefore no finalisation is necessary from wai.annotations.core.stream.util import ProcessState, RequiresNoFinalisation # Type definitions for the streaming callbacks from wai.annotations.core.stream import ThenFunction, DoneFunction class MyProcessorComponent( RequiresNoFinalisation, ProcessorComponent[int, int] # Processes integers into integers ): \"\"\" My processor component, which multiplies incoming integers by values in a schedule, passed on the command-line. \"\"\" # Where we are in the schedule. ProcessState takes an initialiser which is called before processing is started # to reset the state for each run. The initialiser takes the instance it is being used for, i.e. self. Here we # just initialise the index into the schedule to zero at the beginning of each run. schedule_index: int = ProcessState(lambda self: 0) # The option which defines the schedule. schedule: List[int] = TypedOption( \"-s\", \"--schedule\", # The short and long flags to identify the option on the command-line type=int, # The type that will be used to parse the command-line values action=\"concat\", # This flag can be specified multiple times, subsequent values will be appended to the list nargs=\"+\", # Means we can supply one or more values to this flag # choices=[...] # If we wanted to allow choosing only from specific integers # default=[...] # If we had a default schedule in mind. Omission means the default is the empty list required=True, # We need a schedule, the empty list won't do! help=\"The schedule to multiply values by\", # Displayed to the user on usage metavar=\"INT\" # Displayed to the user on usage, describes the type of value that should be supplied ) def process_element( self, element: int, then: ThenFunction[int], done: DoneFunction ): # Multiply the value by the current schedule value result = element * self.schedule[self.schedule_index] # Forward the calculated value then(result) # Move to the next position in the schedule self.schedule_index = (self.schedule_index + 1) % len(self.schedule)","title":"CLI Support in wai-annotations"},{"location":"cli/#cli-support-in-wai-annotations","text":"Command-line options are added to wai-annotations at the component level via the use of Option descriptor classes. This allows for adding command-line options in a declarative style, specifying information about the nature and type of the options, while letting wai-annotations automatically perform the work of getting information from the user and presenting it to the components at run-time. To add an option to a component, create a class-level variable and assign an Option instance to it (see below for a description of the available Option types). To read the value of an option as specified by the user, simply access the class-level variable through the component instance e.g. self.my_option as if it were a property. N.B. Although options are defined at the component level, wai-annotations aggregates them at the stage level. Therefore, care must be taken not to use the same flag/s for multiple options, not only within a component (including its ancestor classes), but also between components that are shared by a stage.","title":"CLI Support in wai-annotations"},{"location":"cli/#types-of-options","text":"","title":"Types of Options"},{"location":"cli/#classoption","text":"An option for selecting a Python class. Is initialised with a class-registry of the available choices the user can select from. The user provides the name of a class in the registry and the class itself is returned.","title":"ClassOption"},{"location":"cli/#countoption","text":"Option which simply returns the number of times its flag was supplied to the command line. This is useful, for example, for verbosity values, where increasing the number of flags increases the logging verbosity (e.g. -vvv ). An optional translation table can be provided which translates the actual value to some mapped value.","title":"CountOption"},{"location":"cli/#flagoption","text":"A simple flag, which evaluates to True when provided on the command-line, or False if not (these semantics can be inverted).","title":"FlagOption"},{"location":"cli/#typedoption","text":"The most versatile option, it takes a type to use to parse the command-line string/s provide by the user. Can return a single-value of that type or a list of values of that type if set to take multiple arguments.","title":"TypedOption"},{"location":"cli/#example","text":"This example creates a processor component which takes a schedule of integers from the command line, and multiplies incoming integers by the next integer in the schedule. # For typing the schedule from typing import List # The type of component we are making from wai.annotations.core.component import ProcessorComponent # or SourceComponent, or SinkComponent # The type of option we want to add to our component from wai.common.cli.options import TypedOption # or ClassOption, or CountOption, or FlagOption # We need some process-state to track where we are in the schedule. We output results as we get them, so no # buffering occurs and therefore no finalisation is necessary from wai.annotations.core.stream.util import ProcessState, RequiresNoFinalisation # Type definitions for the streaming callbacks from wai.annotations.core.stream import ThenFunction, DoneFunction class MyProcessorComponent( RequiresNoFinalisation, ProcessorComponent[int, int] # Processes integers into integers ): \"\"\" My processor component, which multiplies incoming integers by values in a schedule, passed on the command-line. \"\"\" # Where we are in the schedule. ProcessState takes an initialiser which is called before processing is started # to reset the state for each run. The initialiser takes the instance it is being used for, i.e. self. Here we # just initialise the index into the schedule to zero at the beginning of each run. schedule_index: int = ProcessState(lambda self: 0) # The option which defines the schedule. schedule: List[int] = TypedOption( \"-s\", \"--schedule\", # The short and long flags to identify the option on the command-line type=int, # The type that will be used to parse the command-line values action=\"concat\", # This flag can be specified multiple times, subsequent values will be appended to the list nargs=\"+\", # Means we can supply one or more values to this flag # choices=[...] # If we wanted to allow choosing only from specific integers # default=[...] # If we had a default schedule in mind. Omission means the default is the empty list required=True, # We need a schedule, the empty list won't do! help=\"The schedule to multiply values by\", # Displayed to the user on usage metavar=\"INT\" # Displayed to the user on usage, describes the type of value that should be supplied ) def process_element( self, element: int, then: ThenFunction[int], done: DoneFunction ): # Multiply the value by the current schedule value result = element * self.schedule[self.schedule_index] # Forward the calculated value then(result) # Move to the next position in the schedule self.schedule_index = (self.schedule_index + 1) % len(self.schedule)","title":"Example"},{"location":"component_util/","text":"Utility Components wai-annotations comes with some utility base-classes for common types of components, found in the wai.annotations.core.component.util package. They are listed here. Input LocalFilenameSource TODO AnnotationFileProcessor TODO Output LocalFileWriter TODO SeparateFileWriter TODO JSONFileWriter TODO Splitting SplitSink Specialised sink component for handling splitting the stream of processed items among a group of splits. Takes 2 options: --split-names , followed by a name for each split; and --split-ratios , followed by an integer ratio of how many items to add to each split. The sink component will then alternate which split it is acting on behalf of for each item it receives. The alternation is designed so that at any stage, the ratio of items processed for each split is as close as possible to the specified --split-ratios . Sub-classes of SplitSink should implement the consume_element_for_split / finish_split methods, similar to an ordinary sink component. During execution of either of these methods, the following information can be utilised: self.is_splitting : whether the user specified any splits to perform. self.split_label : the label of the current split (specified by --split-names ). This is None if splits weren't specified. The following sections detail additional functionality that can be added to split-sinks. SplitState Specialised form of process state which manages separate state for each split. Accessing this property will return a separate instance depending on which split the component is currently acting for. RequiresNoSplitFinalisation Specialised form of the RequiresNoFinalisation mixin , which declares that the split-sink doesn't need to perform any clean-up after all items have been processed. This automatically implements finish_split . WithPersistentSplitFiles A mixin class for use with components which inherit from SplitSink and need to keep files open while writing them across the splits. The _init_split_files method should be implemented to open all files needed by the sink, and return them as some sort of collection. The _iterate_split_files should iterate over the collection returned by _init_split_files . Components which implement this mixin can then access the _split_files property during execution of their consume_element_for_split / finish_split methods to get access to the file-collection for the current split. Other General-purpose utilities for components. Buffer This processor component buffers the entire stream, and passes it as a list to the next component in its finish method. Enumerator This processor component enumerates each item passed to it, forwarding a tuple of the 0-based index of the item and the item itself. WithRandomness This is a mixin class which can be used with any component, and adds a --seed option for randomisation. The help-text for the option can be specified. If the user sets a seed value, the random property will provide a Random instance initialised with the provided seed. If the option was not set, None will be returned by the property.","title":"Utility Components"},{"location":"component_util/#utility-components","text":"wai-annotations comes with some utility base-classes for common types of components, found in the wai.annotations.core.component.util package. They are listed here.","title":"Utility Components"},{"location":"component_util/#input","text":"","title":"Input"},{"location":"component_util/#localfilenamesource","text":"TODO","title":"LocalFilenameSource"},{"location":"component_util/#annotationfileprocessor","text":"TODO","title":"AnnotationFileProcessor"},{"location":"component_util/#output","text":"","title":"Output"},{"location":"component_util/#localfilewriter","text":"TODO","title":"LocalFileWriter"},{"location":"component_util/#separatefilewriter","text":"TODO","title":"SeparateFileWriter"},{"location":"component_util/#jsonfilewriter","text":"TODO","title":"JSONFileWriter"},{"location":"component_util/#splitting","text":"","title":"Splitting"},{"location":"component_util/#splitsink","text":"Specialised sink component for handling splitting the stream of processed items among a group of splits. Takes 2 options: --split-names , followed by a name for each split; and --split-ratios , followed by an integer ratio of how many items to add to each split. The sink component will then alternate which split it is acting on behalf of for each item it receives. The alternation is designed so that at any stage, the ratio of items processed for each split is as close as possible to the specified --split-ratios . Sub-classes of SplitSink should implement the consume_element_for_split / finish_split methods, similar to an ordinary sink component. During execution of either of these methods, the following information can be utilised: self.is_splitting : whether the user specified any splits to perform. self.split_label : the label of the current split (specified by --split-names ). This is None if splits weren't specified. The following sections detail additional functionality that can be added to split-sinks.","title":"SplitSink"},{"location":"component_util/#splitstate","text":"Specialised form of process state which manages separate state for each split. Accessing this property will return a separate instance depending on which split the component is currently acting for.","title":"SplitState"},{"location":"component_util/#requiresnosplitfinalisation","text":"Specialised form of the RequiresNoFinalisation mixin , which declares that the split-sink doesn't need to perform any clean-up after all items have been processed. This automatically implements finish_split .","title":"RequiresNoSplitFinalisation"},{"location":"component_util/#withpersistentsplitfiles","text":"A mixin class for use with components which inherit from SplitSink and need to keep files open while writing them across the splits. The _init_split_files method should be implemented to open all files needed by the sink, and return them as some sort of collection. The _iterate_split_files should iterate over the collection returned by _init_split_files . Components which implement this mixin can then access the _split_files property during execution of their consume_element_for_split / finish_split methods to get access to the file-collection for the current split.","title":"WithPersistentSplitFiles"},{"location":"component_util/#other","text":"General-purpose utilities for components.","title":"Other"},{"location":"component_util/#buffer","text":"This processor component buffers the entire stream, and passes it as a list to the next component in its finish method.","title":"Buffer"},{"location":"component_util/#enumerator","text":"This processor component enumerates each item passed to it, forwarding a tuple of the 0-based index of the item and the item itself.","title":"Enumerator"},{"location":"component_util/#withrandomness","text":"This is a mixin class which can be used with any component, and adds a --seed option for randomisation. The help-text for the option can be specified. If the user sets a seed value, the random property will provide a Random instance initialised with the provided seed. If the option was not set, None will be returned by the property.","title":"WithRandomness"},{"location":"conversion_options/","text":"Conversion options for wai-annotations The following options can be specified before any stages in the conversion chain, and globally affect the process of converting datasets: -h , --help : Lists this set of conversion options and then exits. -v : Optional argument to set the logging verbosity of the conversion. Can be specified multiple times to further increase verbosity. --macro-file : The macros to use during the conversion.","title":"Conversion options for wai-annotations"},{"location":"conversion_options/#conversion-options-for-wai-annotations","text":"The following options can be specified before any stages in the conversion chain, and globally affect the process of converting datasets: -h , --help : Lists this set of conversion options and then exits. -v : Optional argument to set the logging verbosity of the conversion. Can be specified multiple times to further increase verbosity. --macro-file : The macros to use during the conversion.","title":"Conversion options for wai-annotations"},{"location":"docker/","text":"Rather than installing wai.annotations yourself in virtual environments, you can simply make use of our pre-built Docker images (for an introduction to Docker, please refer to Docker for data scientists ). The following versions of wai.annotations are available as images: 0.7.5: waikatoufdl/wai.annotations:0.7.5 0.7.6: waikatoufdl/wai.annotations:0.7.6 0.7.7: waikatoufdl/wai.annotations:0.7.7 latest: waikatoufdl/wai.annotations:latest (based on latest code in repositories) Example usage The following command-line starts the 0.7.6 version of wai.annotations in interactive mode, mapping the current directory ( pwd ) into the /workspace directory in the container, to have access to the data in this directory: docker run -u $(id -u):$(id -g) -v `pwd`:/workspace -it wai.annotations:0.7.6 Graphical user interface By default, Docker is aimed at worker processes or command-line execution. If you want to make use of graphical interfaces, like displaying images, then you need to do the following: Linux xhost +local:root Add the following arguments to your docker run command: --env=\"DISPLAY\" --volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" Other platforms Please refer to this MOA blog post on how to run graphical user interfaces from within Docker for other platforms. Redis If you want to access a Redis instance outside of the container, then add the following to your docker run command: --net=host","title":"Docker"},{"location":"docker/#example-usage","text":"The following command-line starts the 0.7.6 version of wai.annotations in interactive mode, mapping the current directory ( pwd ) into the /workspace directory in the container, to have access to the data in this directory: docker run -u $(id -u):$(id -g) -v `pwd`:/workspace -it wai.annotations:0.7.6","title":"Example usage"},{"location":"docker/#graphical-user-interface","text":"By default, Docker is aimed at worker processes or command-line execution. If you want to make use of graphical interfaces, like displaying images, then you need to do the following:","title":"Graphical user interface"},{"location":"docker/#linux","text":"xhost +local:root Add the following arguments to your docker run command: --env=\"DISPLAY\" --volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\"","title":"Linux"},{"location":"docker/#other-platforms","text":"Please refer to this MOA blog post on how to run graphical user interfaces from within Docker for other platforms.","title":"Other platforms"},{"location":"docker/#redis","text":"If you want to access a Redis instance outside of the container, then add the following to your docker run command: --net=host","title":"Redis"},{"location":"domains/","text":"Domains Image Classification Domain Images categorised by content. The image classification domain deals with labelling entire images as containing a certain subject. Instances in this domain contain an image and a string label classifying the image. Image Object-Detection Domain Images containing multiple identified objects. The image object-detection domain pertains to finding regions of still images which contain identifiable objects. Instances in this domain consist of an image and a set of regions (either axis-aligned boxes or polygons), each with an accompanying label, identifying the detected objects within the image. Image Segmentation Domain Images segmented by category. The image segmentation domain 'colourises' an image by assigning a category to each pixel (where no category corresponds to 'the background'). Instances in this domain are a still image and a corresponding table of the same size, where each element is a label. Speech Domain Transcriptions of recorded speech. The speech domain covers audio data of people speaking natural languages, annotated with text transcribing the verbal contents of the audio. Instances in this domain are an audio file and a string containing the transcription.","title":"Domains"},{"location":"domains/#domains","text":"","title":"Domains"},{"location":"domains/#image-classification-domain","text":"Images categorised by content. The image classification domain deals with labelling entire images as containing a certain subject. Instances in this domain contain an image and a string label classifying the image.","title":"Image Classification Domain"},{"location":"domains/#image-object-detection-domain","text":"Images containing multiple identified objects. The image object-detection domain pertains to finding regions of still images which contain identifiable objects. Instances in this domain consist of an image and a set of regions (either axis-aligned boxes or polygons), each with an accompanying label, identifying the detected objects within the image.","title":"Image Object-Detection Domain"},{"location":"domains/#image-segmentation-domain","text":"Images segmented by category. The image segmentation domain 'colourises' an image by assigning a category to each pixel (where no category corresponds to 'the background'). Instances in this domain are a still image and a corresponding table of the same size, where each element is a label.","title":"Image Segmentation Domain"},{"location":"domains/#speech-domain","text":"Transcriptions of recorded speech. The speech domain covers audio data of people speaking natural languages, annotated with text transcribing the verbal contents of the audio. Instances in this domain are an audio file and a string containing the transcription.","title":"Speech Domain"},{"location":"examples_formats/","text":"ADAMS to MS-COCO Also adds additional logging information and removes annotations smaller than 5 pixels in either dimension. wai-annotations convert -v \\ from-adams-od \\ -i \"input/*.report\" \\ dimension-discarder \\ --min-width 5 \\ --min-height 5 \\ to-coco-od \\ -o output/annotations.json \\ --license-name \"CC-BY-SA 4.0\" Monolithic Tensorflow records to sharded ones Here we are converting a monolithic TFRecords file into a sharded one ( -s 5 - five shards) and only using a subset of labels ( -l label2,label4,label6 ): wai-annotations convert \\ from-tfrecords-od \\ -i input/objects.records \\ filter-labels \\ -l label2,label4,label6 \\ to-tf-od \\ -o output/subset.records \\ -s 5 \\ -p labels.pbtxt ADAMS to Tensorflow records (masks) The ADAMS input directory contains sub-directories, so we use the \"input/**/*.report\" glob syntax to find all .report files recursively. The data gets split into train/test with a 80/20 ratio. Supplying split names will automatically insert these names into the output file, i.e., output/data.tfrecords will get turned into output/train/data.tfrecords and output/test/data.tfrecords . No path gets supplied to the file containing the labels ( labels.txt ), it will get placed into the correct output directory automatically: wai-annotations convert \\ from-adams-od \\ -i \"input/**/*.report\" \\ coerce-mask \\ to-tf-od \\ -o output/data.tfrecords \\ -p labels.txt \\ --split-names train test \\ --split-ratios 80 20 ADAMS individual layer image segmentation to blue channel JPGs ADAMS supports the individual layers format for image segmentation format, where for each JPG image a PNG with the same file name plus the label suffix is present (e.g.g: 1.jpg -> 1-car.png and 1-person.png). In this case, we only want to include the car annotations in the output. The output gets split into train/val with a ratio of 80/20: wai-annotations convert \\ from-layer-segments-is \\ -i \"input/**/*.png\" \\ --labels car \\ to-indexed-png-is \\ -o output \\ --split-names train val \\ --split-ratios 80 20 VOC to MS-COCO Annotations in VOC format can be converted into MS-COCO ones as follows: wai-annotations convert \\ from-adams-od \\ -i \"input/**/*.xml\" \\ to-coco-od \\ -o output/annotations.json","title":"Format conversions"},{"location":"examples_formats/#adams-to-ms-coco","text":"Also adds additional logging information and removes annotations smaller than 5 pixels in either dimension. wai-annotations convert -v \\ from-adams-od \\ -i \"input/*.report\" \\ dimension-discarder \\ --min-width 5 \\ --min-height 5 \\ to-coco-od \\ -o output/annotations.json \\ --license-name \"CC-BY-SA 4.0\"","title":"ADAMS to MS-COCO"},{"location":"examples_formats/#monolithic-tensorflow-records-to-sharded-ones","text":"Here we are converting a monolithic TFRecords file into a sharded one ( -s 5 - five shards) and only using a subset of labels ( -l label2,label4,label6 ): wai-annotations convert \\ from-tfrecords-od \\ -i input/objects.records \\ filter-labels \\ -l label2,label4,label6 \\ to-tf-od \\ -o output/subset.records \\ -s 5 \\ -p labels.pbtxt","title":"Monolithic Tensorflow records to sharded ones"},{"location":"examples_formats/#adams-to-tensorflow-records-masks","text":"The ADAMS input directory contains sub-directories, so we use the \"input/**/*.report\" glob syntax to find all .report files recursively. The data gets split into train/test with a 80/20 ratio. Supplying split names will automatically insert these names into the output file, i.e., output/data.tfrecords will get turned into output/train/data.tfrecords and output/test/data.tfrecords . No path gets supplied to the file containing the labels ( labels.txt ), it will get placed into the correct output directory automatically: wai-annotations convert \\ from-adams-od \\ -i \"input/**/*.report\" \\ coerce-mask \\ to-tf-od \\ -o output/data.tfrecords \\ -p labels.txt \\ --split-names train test \\ --split-ratios 80 20","title":"ADAMS to Tensorflow records (masks)"},{"location":"examples_formats/#adams-individual-layer-image-segmentation-to-blue-channel-jpgs","text":"ADAMS supports the individual layers format for image segmentation format, where for each JPG image a PNG with the same file name plus the label suffix is present (e.g.g: 1.jpg -> 1-car.png and 1-person.png). In this case, we only want to include the car annotations in the output. The output gets split into train/val with a ratio of 80/20: wai-annotations convert \\ from-layer-segments-is \\ -i \"input/**/*.png\" \\ --labels car \\ to-indexed-png-is \\ -o output \\ --split-names train val \\ --split-ratios 80 20","title":"ADAMS individual layer image segmentation to blue channel JPGs"},{"location":"examples_formats/#voc-to-ms-coco","text":"Annotations in VOC format can be converted into MS-COCO ones as follows: wai-annotations convert \\ from-adams-od \\ -i \"input/**/*.xml\" \\ to-coco-od \\ -o output/annotations.json","title":"VOC to MS-COCO"},{"location":"examples_imgaug/","text":"The following command-line loads ADAMS annotations and saves augmented MSCOCO ones. The pipeline adds augmented images ( -m add ) that are cropped, flipped, blurred, rotated and scaled. Each inline stream processor changes randomly about 10% of the images ( -T 0.9 - if a random number between 0 and 1 is at least the threshold provided with the -T parameter then the augmentation occurs). Since each augmentation has its own stochastic element, it is being seeded ( -a ) to generate reproducible datasets: wai-annotations convert \\ from-adams-od \\ -i \"input/**/*.report\" \\ crop \\ -a -m add -T 0.9 -f 0.0 -t 0.1 \\ # crop 0-10% of the image flip \\ -a -m add -T 0.9 -d lr \\ # flip left-to-right gaussian-blur \\ -a -m add -T 0.9 -f 0.2 -t 0.5 \\ # sigma from 0.2-0.5 rotate \\ -a -m add -T 0.9 -f=-10 -t=10 \\ # rotate from -10 to +10 degrees scale \\ -a -m add -T 0.9 -f 0.8 -t 1.2 -k -u \\ # scale from 80 to 120%, keeping aspect ratio, resizing the image to-coco-od \\ -o output/annotations.json","title":"Image dataset augmentation"},{"location":"examples_imgstats/","text":"Label distribution for image classification dataset The following generates a label distribution (using percentages) for the 17flowers image classification dataset ( zip ), saving the result in a JSON file: wai-annotations convert \\ from-subdir-ic \\ -i \"./17flowers/subdir/**/*.jpg\" \\ label-dist-ic \\ -f json \\ -o ./17flowers-labeldist-ic.json The JSON file will look something like this: { \"Windflower\": 63, \"Fritillary\": 65, \"Tigerlily\": 50, \"Tulip\": 41, \"Daffodil\": 71, \"Crocus\": 50, \"Sunflower\": 71, \"Buttercup\": 54, \"Daisy\": 57, \"Bluebell\": 28, \"Snowdrop\": 50, \"Dandelion\": 43, \"ColtsFoot\": 55, \"Iris\": 77, \"Pansy\": 56, \"LilyValley\": 17 } Label distribution for object detection dataset The following generates a label distribution (using percentages) for the 17flowers object detection dataset ( zip ), saving the result in a JSON file: wai-annotations convert \\ from-voc-od \\ -i \"./17flowers/voc/*.xml\" \\ label-dist-od \\ -f json \\ -p \\ -o ./17flowers-labeldist-od.json The JSON file will look similar to this: { \"Daffodil\": 8.372641509433961, \"Pansy\": 6.60377358490566, \"Buttercup\": 6.367924528301887, \"Tigerlily\": 5.89622641509434, \"Daisy\": 6.721698113207547, \"Bluebell\": 3.30188679245283, \"Tulip\": 4.834905660377359, \"Iris\": 9.080188679245282, \"Fritillary\": 7.665094339622641, \"Dandelion\": 5.070754716981132, \"ColtsFoot\": 6.485849056603773, \"Crocus\": 5.89622641509434, \"Sunflower\": 8.372641509433961, \"Snowdrop\": 5.89622641509434, \"Windflower\": 7.429245283018868, \"LilyValley\": 2.0047169811320753 }","title":"Image dataset statistics"},{"location":"examples_imgstats/#label-distribution-for-image-classification-dataset","text":"The following generates a label distribution (using percentages) for the 17flowers image classification dataset ( zip ), saving the result in a JSON file: wai-annotations convert \\ from-subdir-ic \\ -i \"./17flowers/subdir/**/*.jpg\" \\ label-dist-ic \\ -f json \\ -o ./17flowers-labeldist-ic.json The JSON file will look something like this: { \"Windflower\": 63, \"Fritillary\": 65, \"Tigerlily\": 50, \"Tulip\": 41, \"Daffodil\": 71, \"Crocus\": 50, \"Sunflower\": 71, \"Buttercup\": 54, \"Daisy\": 57, \"Bluebell\": 28, \"Snowdrop\": 50, \"Dandelion\": 43, \"ColtsFoot\": 55, \"Iris\": 77, \"Pansy\": 56, \"LilyValley\": 17 }","title":"Label distribution for image classification dataset"},{"location":"examples_imgstats/#label-distribution-for-object-detection-dataset","text":"The following generates a label distribution (using percentages) for the 17flowers object detection dataset ( zip ), saving the result in a JSON file: wai-annotations convert \\ from-voc-od \\ -i \"./17flowers/voc/*.xml\" \\ label-dist-od \\ -f json \\ -p \\ -o ./17flowers-labeldist-od.json The JSON file will look similar to this: { \"Daffodil\": 8.372641509433961, \"Pansy\": 6.60377358490566, \"Buttercup\": 6.367924528301887, \"Tigerlily\": 5.89622641509434, \"Daisy\": 6.721698113207547, \"Bluebell\": 3.30188679245283, \"Tulip\": 4.834905660377359, \"Iris\": 9.080188679245282, \"Fritillary\": 7.665094339622641, \"Dandelion\": 5.070754716981132, \"ColtsFoot\": 6.485849056603773, \"Crocus\": 5.89622641509434, \"Sunflower\": 8.372641509433961, \"Snowdrop\": 5.89622641509434, \"Windflower\": 7.429245283018868, \"LilyValley\": 2.0047169811320753 }","title":"Label distribution for object detection dataset"},{"location":"examples_imgvis/","text":"Combine annotations in single image In order to determine whether a model does not have a bias regarding to the location of annotations when training, the to-annotation-overlay-od plugin can generate a single image that shows the outlines of all the annotations (bbox or polygon). If images have different sizes (e.g., from different data collection batches), scaling the images and annotations to a specific size, like the one a model uses internally is, recommended. The following example overlays all the bboxes from the 17flowers object detection dataset ( zip ) on a single image of 640x480 pixels: wai-annotations convert \\ from-voc-od \\ -i \"./17flowers/voc/*.xml\" \\ to-annotation-overlay-od \\ -c 255,0,0,32 \\ -s 640,480 \\ -o ./to-annotation-overlay-od.png Overlay annotations and display them The following loads an object detection dataset, overlays the annotations and displays them in a window: wai-annotations convert \\ from-voc-od \\ -i \"./17flowers/voc/*.xml\" \\ add-annotation-overlay-od \\ --vary-colors \\ --outline-alpha 255 \\ --outline-thickness 3 \\ --fill \\ --fill-alpha 128 \\ --font-size 24 \\ --text-placement T,L \\ --force-bbox \\ image-viewer-od \\ --size 1024,768 \\ --position 300,150 \\ --delay 200","title":"Image dataset visualizations"},{"location":"examples_imgvis/#combine-annotations-in-single-image","text":"In order to determine whether a model does not have a bias regarding to the location of annotations when training, the to-annotation-overlay-od plugin can generate a single image that shows the outlines of all the annotations (bbox or polygon). If images have different sizes (e.g., from different data collection batches), scaling the images and annotations to a specific size, like the one a model uses internally is, recommended. The following example overlays all the bboxes from the 17flowers object detection dataset ( zip ) on a single image of 640x480 pixels: wai-annotations convert \\ from-voc-od \\ -i \"./17flowers/voc/*.xml\" \\ to-annotation-overlay-od \\ -c 255,0,0,32 \\ -s 640,480 \\ -o ./to-annotation-overlay-od.png","title":"Combine annotations in single image"},{"location":"examples_imgvis/#overlay-annotations-and-display-them","text":"The following loads an object detection dataset, overlays the annotations and displays them in a window: wai-annotations convert \\ from-voc-od \\ -i \"./17flowers/voc/*.xml\" \\ add-annotation-overlay-od \\ --vary-colors \\ --outline-alpha 255 \\ --outline-thickness 3 \\ --fill \\ --fill-alpha 128 \\ --font-size 24 \\ --text-placement T,L \\ --force-bbox \\ image-viewer-od \\ --size 1024,768 \\ --position 300,150 \\ --delay 200","title":"Overlay annotations and display them"},{"location":"examples_overview/","text":"Examples are grouped into the following topics: Format conversions Image dataset augmentation Image dataset statistics Image dataset visualizations Video handling Predictions via Redis","title":"Overview"},{"location":"examples_redis_predictions/","text":"Redis is a fast, in-memory data store, which can be used as a database, cache, streaming engine, and message broker. It is a great way of incorporating deep learning frameworks that are run from within Docker containers, simplifying the dependencies due to decoupling. wai.annotations has support for the following data domains: image classification : sends image, receives JSON with label/probability pairs object detection : sends image, receives JSON in OPEX format image segmentation : sends image, receives segmented image (indexed PNG or blue-channel PNG) Docker images The following Docker images are available for connecting to the prediction plugins: Image classification PyTorch Tensorflow Object detection Detectron2 Yolov5 Image segmentation Image segmentation Keras Object detection example In this example we are using a prebuilt yolov5 model (using MS-COCO) to make predictions on the frames that come from a dashcam video, overlay the predictions on the images and display them. For the model and wai.annotations we will be using existing docker containers. NB: No GPU required. Data Input Output Preparation NB: Place all the downloads in the current directory Download the dashcam01.mp4 video from the BoofCV project Download the yolo5n.pt model Download the coco.yaml data description for the yolo5n model The host machine must have a Redis server instance running. Two options: Install it natively via sudo apt-get install redis (and then restart it with sudo systemctl restart redis ) Spin up a docker container with: docker run --net=host --name redis-server -d redis Yolov5 model The following command launches a Yolov5 model via the container's yolov5_predict_redis command, running on the CPU: docker run \\ --net=host -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/pytorch-yolov5:2022-01-21_cpu \\ yolov5_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --model /workspace/yolov5n.pt \\ --data /workspace/coco.yaml wai.annotations The following wai.annotations pipeline loads every 2nd frame from the dashcam01.mp4 video, obtains predictions from the Yolov5 model (using the Redis backend), overlays the predictions and then displays them: docker run -u $(id -u):$(id -g) \\ --net=host \\ --env=\"DISPLAY\" --volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" \\ -v `pwd`:/workspace \\ -it waikatoufdl/wai.annotations:0.7.6 \\ wai-annotations convert \\ from-video-file-od \\ -i /workspace/dashcam01.mp4 \\ -n 2 \\ redis-predict-od \\ --channel-out images \\ --channel-in predictions \\ --timeout 1.0 \\ add-annotation-overlay-od \\ --outline-alpha 255 \\ --outline-thickness 1 \\ --fill \\ --fill-alpha 128 \\ --vary-colors \\ --font-size 10 \\ --text-format \"{label}: {score}\" \\ --text-placement T,L \\ --force-bbox \\ image-viewer-od \\ --size 800,224 \\ --delay 1","title":"Predictions via Redis"},{"location":"examples_redis_predictions/#docker-images","text":"The following Docker images are available for connecting to the prediction plugins: Image classification PyTorch Tensorflow Object detection Detectron2 Yolov5 Image segmentation Image segmentation Keras","title":"Docker images"},{"location":"examples_redis_predictions/#object-detection-example","text":"In this example we are using a prebuilt yolov5 model (using MS-COCO) to make predictions on the frames that come from a dashcam video, overlay the predictions on the images and display them. For the model and wai.annotations we will be using existing docker containers. NB: No GPU required.","title":"Object detection example"},{"location":"examples_redis_predictions/#data","text":"","title":"Data"},{"location":"examples_redis_predictions/#input","text":"","title":"Input"},{"location":"examples_redis_predictions/#output","text":"","title":"Output"},{"location":"examples_redis_predictions/#preparation","text":"NB: Place all the downloads in the current directory Download the dashcam01.mp4 video from the BoofCV project Download the yolo5n.pt model Download the coco.yaml data description for the yolo5n model The host machine must have a Redis server instance running. Two options: Install it natively via sudo apt-get install redis (and then restart it with sudo systemctl restart redis ) Spin up a docker container with: docker run --net=host --name redis-server -d redis","title":"Preparation"},{"location":"examples_redis_predictions/#yolov5-model","text":"The following command launches a Yolov5 model via the container's yolov5_predict_redis command, running on the CPU: docker run \\ --net=host -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/pytorch-yolov5:2022-01-21_cpu \\ yolov5_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --model /workspace/yolov5n.pt \\ --data /workspace/coco.yaml","title":"Yolov5 model"},{"location":"examples_redis_predictions/#waiannotations","text":"The following wai.annotations pipeline loads every 2nd frame from the dashcam01.mp4 video, obtains predictions from the Yolov5 model (using the Redis backend), overlays the predictions and then displays them: docker run -u $(id -u):$(id -g) \\ --net=host \\ --env=\"DISPLAY\" --volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" \\ -v `pwd`:/workspace \\ -it waikatoufdl/wai.annotations:0.7.6 \\ wai-annotations convert \\ from-video-file-od \\ -i /workspace/dashcam01.mp4 \\ -n 2 \\ redis-predict-od \\ --channel-out images \\ --channel-in predictions \\ --timeout 1.0 \\ add-annotation-overlay-od \\ --outline-alpha 255 \\ --outline-thickness 1 \\ --fill \\ --fill-alpha 128 \\ --vary-colors \\ --font-size 10 \\ --text-format \"{label}: {score}\" \\ --text-placement T,L \\ --force-bbox \\ image-viewer-od \\ --size 800,224 \\ --delay 1","title":"wai.annotations"},{"location":"examples_video/","text":"Read frames from a video file The following command retrieves every 10th frame ( -n ) from the input video, will process a maximum of 100 frames ( -m ) and will process frames starting from 1000 ( -f ) and display the frames ( example video ): wai-annotations convert \\ from-video-file-od \\ -i \"./video/dashcam01.mp4\" \\ -n 10 \\ -m 100 \\ -f 1000 \\ image-viewer-od Read frames from a webcam A webcam can also be used for obtaining frames, as the following example demonstrates (uses webcam with ID 0 ): wai-annotations convert \\ from-webcam-od \\ -i 0 \\ -n 10 \\ -m 100 \\ -f 100 image-viewer-od Generate a MJPEG video file of dissimilar frames Videos contain a lot of frames and outputting or processing all of them is not a viable option in most cases. For that purpose, the skip-similar-frames plugin can be used for dropping frames that are too similar. The -t/--change-threshold parameter determines how sensitive to change the plugin is (ratio of pixels that are different between images). The subset of frames is written to a MJPEG video file with 1 frame/sec: wai-annotations convert \\ from-video-file-od \\ -i \"./video/some_video.mp4\" \\ skip-similar-frames \\ --change-threshold 0.03 \\ --verbose \\ to-video-file-od \\ -f 1 \\ -o ./out.mjpeg","title":"Video handling"},{"location":"examples_video/#read-frames-from-a-video-file","text":"The following command retrieves every 10th frame ( -n ) from the input video, will process a maximum of 100 frames ( -m ) and will process frames starting from 1000 ( -f ) and display the frames ( example video ): wai-annotations convert \\ from-video-file-od \\ -i \"./video/dashcam01.mp4\" \\ -n 10 \\ -m 100 \\ -f 1000 \\ image-viewer-od","title":"Read frames from a video file"},{"location":"examples_video/#read-frames-from-a-webcam","text":"A webcam can also be used for obtaining frames, as the following example demonstrates (uses webcam with ID 0 ): wai-annotations convert \\ from-webcam-od \\ -i 0 \\ -n 10 \\ -m 100 \\ -f 100 image-viewer-od","title":"Read frames from a webcam"},{"location":"examples_video/#generate-a-mjpeg-video-file-of-dissimilar-frames","text":"Videos contain a lot of frames and outputting or processing all of them is not a viable option in most cases. For that purpose, the skip-similar-frames plugin can be used for dropping frames that are too similar. The -t/--change-threshold parameter determines how sensitive to change the plugin is (ratio of pixels that are different between images). The subset of frames is written to a MJPEG video file with 1 frame/sec: wai-annotations convert \\ from-video-file-od \\ -i \"./video/some_video.mp4\" \\ skip-similar-frames \\ --change-threshold 0.03 \\ --verbose \\ to-video-file-od \\ -f 1 \\ -o ./out.mjpeg","title":"Generate a MJPEG video file of dissimilar frames"},{"location":"glossary/","text":"Glossary Glossary of terminology used in wai-annotations. Component - A single participating element in a pipeline. Can be a source (inserts items into the start of a pipeline), a sink (consumes items from the end of a pipeline), or a processor (an intermediary element which performs some operation on the items in the pipeline). Conversion Pipeline - A series of stages comprising a complete conversion sequence, consisting of an input format, a series of intermediate processing stages, and a final output format. Cross-Domain Converter (XDC) - An intermediate processor which converts a dataset from one domain to another. For example, a video-based domain could be converted to an image-based domain by treating each frame of the video as an individual image. Domain - A specific type of data, being annotated in a specific manner. For example, datasets in the image object-detection domain consist of still images annotated with regions containing identified objects. Format - An external representation of a domain. This is typically a way of storing instances of the domain on disk. Inline Stream Processor (ISP) - An intermediate processor in the conversion chain, which performs some mutation of the items in a dataset. ISPs cannot change the domain of a dataset. For example, an ISP might remove images that are smaller than a certain size from the conversion stream (for image-based domains). Instance - A specific example of data-item and its annotations in a given domain. Macro - A command-line keyword which is used in place of a series of command-line options. Macros are stored in a JSON file, and specified to wai.annotations via the --macro-file command-line option. Negative Example - An item in a dataset which has no annotations. These are typically used when learning to provide examples of what not to look for. Plugin - An implementation of a stage which can be used by wai.annotations to perform some feature. Plugins can be specified by external modules via the wai.annotations.plugins entry-point in their setup.py script. Plugin Specifier - A class which advertises the components that a particular plugin offers for use with wai-annotations. Pipeline - A series of components which process items in their defined order, each passing its output to the next. Specifier - A class used to advertise a stage/domain to wai.annotations from an external module. Splitting - Because wai.annotations only supports linear pipelines, many formats support splitting of their outputs over a number of output directories. Stage - A collection of components which produces, consumes or processes instances.","title":"Glossary"},{"location":"glossary/#glossary","text":"Glossary of terminology used in wai-annotations. Component - A single participating element in a pipeline. Can be a source (inserts items into the start of a pipeline), a sink (consumes items from the end of a pipeline), or a processor (an intermediary element which performs some operation on the items in the pipeline). Conversion Pipeline - A series of stages comprising a complete conversion sequence, consisting of an input format, a series of intermediate processing stages, and a final output format. Cross-Domain Converter (XDC) - An intermediate processor which converts a dataset from one domain to another. For example, a video-based domain could be converted to an image-based domain by treating each frame of the video as an individual image. Domain - A specific type of data, being annotated in a specific manner. For example, datasets in the image object-detection domain consist of still images annotated with regions containing identified objects. Format - An external representation of a domain. This is typically a way of storing instances of the domain on disk. Inline Stream Processor (ISP) - An intermediate processor in the conversion chain, which performs some mutation of the items in a dataset. ISPs cannot change the domain of a dataset. For example, an ISP might remove images that are smaller than a certain size from the conversion stream (for image-based domains). Instance - A specific example of data-item and its annotations in a given domain. Macro - A command-line keyword which is used in place of a series of command-line options. Macros are stored in a JSON file, and specified to wai.annotations via the --macro-file command-line option. Negative Example - An item in a dataset which has no annotations. These are typically used when learning to provide examples of what not to look for. Plugin - An implementation of a stage which can be used by wai.annotations to perform some feature. Plugins can be specified by external modules via the wai.annotations.plugins entry-point in their setup.py script. Plugin Specifier - A class which advertises the components that a particular plugin offers for use with wai-annotations. Pipeline - A series of components which process items in their defined order, each passing its output to the next. Specifier - A class used to advertise a stage/domain to wai.annotations from an external module. Splitting - Because wai.annotations only supports linear pipelines, many formats support splitting of their outputs over a number of output directories. Stage - A collection of components which produces, consumes or processes instances.","title":"Glossary"},{"location":"install/","text":"Installation of wai-annotations To install wai-annotations issue the following commands using pip: pip install wai.annotations.core wai.annotations.core does not come with any input/output formats installed. You will need to install a plugin for each format that you want to convert between. For a description of the plugin system, see the plugin guide . For installing all available plugins, use this command: pip install wai.annotations Available plugins Image classification https://github.com/waikato-ufdl/wai-annotations-adams https://github.com/waikato-ufdl/wai-annotations-subdir Object detection https://github.com/waikato-ufdl/wai-annotations-adams https://github.com/waikato-ufdl/wai-annotations-coco https://github.com/waikato-ufdl/wai-annotations-opex https://github.com/waikato-ufdl/wai-annotations-roi https://github.com/waikato-ufdl/wai-annotations-tf https://github.com/waikato-ufdl/wai-annotations-vgg https://github.com/waikato-ufdl/wai-annotations-voc https://github.com/waikato-ufdl/wai-annotations-yolo Image segmentation https://github.com/waikato-ufdl/wai-annotations-bluechannel https://github.com/waikato-ufdl/wai-annotations-grayscale https://github.com/waikato-ufdl/wai-annotations-indexedpng https://github.com/waikato-ufdl/wai-annotations-layersegments Audio https://github.com/waikato-ufdl/wai-annotations-audio https://github.com/waikato-ufdl/wai-annotations-commonvoice https://github.com/waikato-ufdl/wai-annotations-festvox Miscellaneous https://github.com/waikato-ufdl/wai-annotations-generic - generic plugins for wrapping user classes https://github.com/waikato-ufdl/wai-annotations-imgaug - image augmentation https://github.com/waikato-ufdl/wai-annotations-imgstats - image/label statistics https://github.com/waikato-ufdl/wai-annotations-imgvis - image visualization https://github.com/waikato-ufdl/wai-annotations-redis-predictions - making predictions via Redis backend https://github.com/waikato-ufdl/wai-annotations-video - video support","title":"Installation"},{"location":"install/#installation-of-wai-annotations","text":"To install wai-annotations issue the following commands using pip: pip install wai.annotations.core wai.annotations.core does not come with any input/output formats installed. You will need to install a plugin for each format that you want to convert between. For a description of the plugin system, see the plugin guide . For installing all available plugins, use this command: pip install wai.annotations","title":"Installation of wai-annotations"},{"location":"install/#available-plugins","text":"Image classification https://github.com/waikato-ufdl/wai-annotations-adams https://github.com/waikato-ufdl/wai-annotations-subdir Object detection https://github.com/waikato-ufdl/wai-annotations-adams https://github.com/waikato-ufdl/wai-annotations-coco https://github.com/waikato-ufdl/wai-annotations-opex https://github.com/waikato-ufdl/wai-annotations-roi https://github.com/waikato-ufdl/wai-annotations-tf https://github.com/waikato-ufdl/wai-annotations-vgg https://github.com/waikato-ufdl/wai-annotations-voc https://github.com/waikato-ufdl/wai-annotations-yolo Image segmentation https://github.com/waikato-ufdl/wai-annotations-bluechannel https://github.com/waikato-ufdl/wai-annotations-grayscale https://github.com/waikato-ufdl/wai-annotations-indexedpng https://github.com/waikato-ufdl/wai-annotations-layersegments Audio https://github.com/waikato-ufdl/wai-annotations-audio https://github.com/waikato-ufdl/wai-annotations-commonvoice https://github.com/waikato-ufdl/wai-annotations-festvox Miscellaneous https://github.com/waikato-ufdl/wai-annotations-generic - generic plugins for wrapping user classes https://github.com/waikato-ufdl/wai-annotations-imgaug - image augmentation https://github.com/waikato-ufdl/wai-annotations-imgstats - image/label statistics https://github.com/waikato-ufdl/wai-annotations-imgvis - image visualization https://github.com/waikato-ufdl/wai-annotations-redis-predictions - making predictions via Redis backend https://github.com/waikato-ufdl/wai-annotations-video - video support","title":"Available plugins"},{"location":"plugin/","text":"Adding components to wai-annotations through plugins wai-annotations uses a plugin system to allow other libraries to add new processing stages without modifying the base library source. This document details how to go about using this system. Plugin Types There are 3 types of stage which can be added to wai-annotations via plugin: input formats, processing stages and output formats. New domains can also be added, but these are specified indirectly via any of the previous components. Domains New domains are added indirectly to wai-annotations as dependencies of components, as a domain which has no components is essentially unreachable to a conversion chain. However, the specification of new domains is treated as a plugin-related issue, so it is detailed here. Definition of a new domain is a multi-step process, as follows. Define the type of the (unannotated) data for the domain, as a sub-class of Data . Objects of this class hold the name and binary data for an instance, along with any additional meta-data. Define the type of the annotations for an instance in the domain. This can be any arbitrary type. Define the type of instances in the domain as a sub-class of Instance . This represents the association between a data-item and its annotations. At its simplest, this is just a container for the data-item and the annotations, but additional meta-data about the relation can be added for specific domains, if required. Define a specifier class for the domain, sub-classing DomainSpecifier . This advertises the domain to the wai-annotations framework. Example from typing import Type # Import the base classes from wai.annotations.core.domain import Data, Instance, DomainSpecifier # Define the data-type which holds dataset item data class MyDomainData(Data): @classmethod def from_file_data(cls, file_name: str, file_data: bytes) -> 'MyFileInfo': # If we don't need any additional information, or it is calculated in the init method return cls(file_name, file_data) # Define the type of annotations for the domain class MyDomainAnnotations: ... # Define an instance type, adding additional functionality, if required class MyDomainInstance(Instance[MyDomainData, MyDomainAnnotations]): @classmethod def data_type(cls) -> Type[MyDomainData]: return MyDomainData @classmethod def annotations_type(cls) -> Type[MyDomainAnnotations]: return MyDomainAnnotations def additional_method(self): ... # Define the domain specifier reporting the various classes for domain instances class MyDomainSpecifier(DomainSpecifier): @classmethod def domain_name(cls) -> str: return \"my domain\" @classmethod def description(cls) -> str: return \"A detailed description of the domain\" @classmethod def data_type(cls) -> Type[MyDomainData]: return MyDomainData @classmethod def annotations_type(cls) -> Type[MyDomainAnnotations]: return MyDomainAnnotations @classmethod def instance_class(cls) -> Type[MyDomainInstance]: return MyDomainInstance Stages Stages are a partial pipeline of components. Input stages consist of a source component and zero or more processing components, processing stages only consist of processing components, and output stages consist of zero or more processing components followed by a sink component. To create a new stage, the individual components must be created and then advertised through a stage specifier class. The base classes for components can be imported from wai-annotations' core package: from wai.annotations.core.component import SourceComponent from wai.annotations.core.component import ProcessorComponent from wai.annotations.core.component import SinkComponent Sub-class the base classes for the types of components you are trying to implement, and fill in the generic type-parameters and abstract methods. See the examples below for guidance. Then sub-class one of the stage-specifier classes for the type of stage you are creating: from wai.annotations.core.specifier import SourceStageSpecifier from wai.annotations.core.specifier import ProcessorStageSpecifier from wai.annotations.core.specifier import SinkStageSpecifier The specifier classes require that you override the description method, which provides a description of the stage, and the components method, which lists the components which comprise the stage. Source/Sink stages also require a domain method, which returns the domain that the stage reads/writes. Processor stages instead have a domain_transfer_function method, which returns the output domain for a given input domain (this way, processing stages can be used in multiple domains). In order for wai-annotations to recognise your plugin, the specifier class needs to be advertised as an entry point in your setup script under the wai.annotations.plugins group: # setup.py from setuptools import setup setup( ..., entry_points={ \"wai.annotations.plugins\": [ # Input stage \"from-my-input-format=com.example.specifiers:MySourceStageSpecifier\", # Output stage \"to-my-output-format=com.example.specifiers:MySinkStagepecifier\", # Processor stages \"my-processor=com.example.specifiers:MyProcessorStageSpecifier\" ] } ) Example Input Stage Input stages consist of a source component followed by zero or more processing components. Typically the source component will be wai.annotations.core.component.util.LocalFilenameSource , but this is not required. What is required is that the final component of the stage outputs instances in the domain that the stage operates. from typing import Type, Tuple from wai.annotations.core.component import SourceComponent, ProcessorComponent from wai.annotations.domain.image.object_detection import ( ImageObjectDetectionInstance, ImageObjectDetectionDomainSpecifier ) from wai.annotations.core.specifier import SourceStageSpecifier from wai.annotations.core.stream import ThenFunction, DoneFunction # Define the source component (generic type is the type this source produces) class MySourceComponent(SourceComponent[str]): # Any command-line options here... def produce( self, then: ThenFunction[str], done: DoneFunction ): # Call then(str) multiple times... ... # Call done() done() # Define a processor component to parse each string from the source component into a domain instance # (generic types are input and output type respectively) class MyProcessorComponent(ProcessorComponent[str, ImageObjectDetectionInstance]): # Any command-line options here... def process_element( self, element: str, then: ThenFunction[ImageObjectDetectionInstance], done: DoneFunction ): # Parse each string and forward then(self._parse(element)) def finish( self, then: ThenFunction[ImageObjectDetectionInstance], done: DoneFunction ): # Perform any clean-up ... # Call done done() def _parse(self, element: str) -> ImageObjectDetectionInstance: # Parse the string into an instance ... # Create the specifier class for the stage class MySourceStageSpecifier(SourceStageSpecifier): @classmethod def description(cls) -> str: return \"My source stage\" @classmethod def components(cls) -> Tuple[Type[MySourceComponent], Type[MyProcessorComponent]]: return MySourceComponent, MyProcessorComponent @classmethod def domain(cls) -> Type[ImageObjectDetectionDomainSpecifier]: return ImageObjectDetectionDomainSpecifier Example Output Stage Output stages consist of zero or more processing components followed by a single sink component. The first component of the stage must take instances in the stage's domain as input. from typing import Type, Tuple from wai.annotations.core.component import ProcessorComponent, SinkComponent from wai.annotations.domain.image.object_detection import ( ImageObjectDetectionInstance, ImageObjectDetectionDomainSpecifier ) from wai.annotations.core.specifier import SinkStageSpecifier from wai.annotations.core.stream import ThenFunction, DoneFunction # Define a processor component to format each domain instance into a string # (generic types are input and output type respectively) class MyProcessorComponent(ProcessorComponent[ImageObjectDetectionInstance, str]): # Any command-line options here... def process_element( self, element: ImageObjectDetectionInstance, then: ThenFunction[str], done: DoneFunction ): # Format each instance and forward then(self._format(element)) def finish( self, then: ThenFunction[ImageObjectDetectionInstance], done: DoneFunction ): # Perform any clean-up ... # Call done done() def _format(self, element: ImageObjectDetectionInstance) -> str: # Format the instance into an str ... # Define the sink component (generic type is the type this sink consumes) class MySinkComponent(SinkComponent[str]): # Any command-line options here... def consume_element(self, element: str): # E.g. write the string to disk ... def finish(self): # Any tidy-up ... # Create the specifier class for the stage class MySinkStageSpecifier(SinkStageSpecifier): @classmethod def description(cls) -> str: return \"My source stage\" @classmethod def components(cls) -> Tuple[Type[MyProcessorComponent], Type[MySinkComponent]]: return MyProcessorComponent, MySinkComponent @classmethod def domain(cls) -> Type[ImageObjectDetectionDomainSpecifier]: return ImageObjectDetectionDomainSpecifier Example Processor Stage Processor stages consist of one or more processing components. The domain-transfer function defined on the specifier must match the input/output types of the chain of components for the stage. from typing import Type, Tuple from wai.annotations.core.component import ProcessorComponent from wai.annotations.core.domain import Instance, DomainSpecifier from wai.annotations.core.specifier import ProcessorStageSpecifier from wai.annotations.core.stream import ThenFunction, DoneFunction from wai.annotations.core.stream.util import ProcessState # Define a processor component to remove every second instance (for any domain) # (generic types are input and output type respectively) class MyProcessorComponent(ProcessorComponent[Instance, Instance]): # Any command-line options here... # Process-state to track whether we need to remove the next instance remove_next: bool = ProcessState(lambda self: False) def process_element( self, element: Instance, then: ThenFunction[Instance], done: DoneFunction ): if not self.remove_next: then(element) self.remove_next = not self.remove_next def finish( self, then: ThenFunction[Instance], done: DoneFunction ): # Perform any clean-up ... # Call done done() # Create the specifier class for the stage class MyProcessorStageSpecifier(ProcessorStageSpecifier): @classmethod def description(cls) -> str: return \"My source stage\" @classmethod def components(cls) -> Tuple[Type[MyProcessorComponent]]: return MyProcessorComponent, @classmethod def domain_transfer_function( cls, input_domain: Type[DomainSpecifier] ) -> Type[DomainSpecifier]: # Because our processor stage works in any domain, and does not cross domains, just return the input domain return input_domain Best Practice Although in each of the examples shown here, we have defined our plugin specifiers in the same file as the components they advertise, this is not the recommended approach. The specifier types should instead be defined in their own sub-package, and the methods should locally import the specified types (instead of globally at the beginning of the specifier Python file). This is so the specifier can be imported into the plugin system without importing potentially heavy-weight libraries that the components depend on for their functionality. This way the system can provide reflection of the available plugins, but only load those plugins that are actually selected for use in a conversion. Adding Command-Line Options to Plugin Components See here for a description of command-line option support in wai-annotations. Splitting Sink-components (and by extension sink-stages) may require that the incoming instances be split across a number of output locations. Special support for splitting is provided in the wai.annotations.core.component.util sub-package. See here for information on how to add splitting to your sink-stages.","title":"Plugin guide"},{"location":"plugin/#adding-components-to-wai-annotations-through-plugins","text":"wai-annotations uses a plugin system to allow other libraries to add new processing stages without modifying the base library source. This document details how to go about using this system.","title":"Adding components to wai-annotations through plugins"},{"location":"plugin/#plugin-types","text":"There are 3 types of stage which can be added to wai-annotations via plugin: input formats, processing stages and output formats. New domains can also be added, but these are specified indirectly via any of the previous components.","title":"Plugin Types"},{"location":"plugin/#domains","text":"New domains are added indirectly to wai-annotations as dependencies of components, as a domain which has no components is essentially unreachable to a conversion chain. However, the specification of new domains is treated as a plugin-related issue, so it is detailed here. Definition of a new domain is a multi-step process, as follows. Define the type of the (unannotated) data for the domain, as a sub-class of Data . Objects of this class hold the name and binary data for an instance, along with any additional meta-data. Define the type of the annotations for an instance in the domain. This can be any arbitrary type. Define the type of instances in the domain as a sub-class of Instance . This represents the association between a data-item and its annotations. At its simplest, this is just a container for the data-item and the annotations, but additional meta-data about the relation can be added for specific domains, if required. Define a specifier class for the domain, sub-classing DomainSpecifier . This advertises the domain to the wai-annotations framework.","title":"Domains"},{"location":"plugin/#example","text":"from typing import Type # Import the base classes from wai.annotations.core.domain import Data, Instance, DomainSpecifier # Define the data-type which holds dataset item data class MyDomainData(Data): @classmethod def from_file_data(cls, file_name: str, file_data: bytes) -> 'MyFileInfo': # If we don't need any additional information, or it is calculated in the init method return cls(file_name, file_data) # Define the type of annotations for the domain class MyDomainAnnotations: ... # Define an instance type, adding additional functionality, if required class MyDomainInstance(Instance[MyDomainData, MyDomainAnnotations]): @classmethod def data_type(cls) -> Type[MyDomainData]: return MyDomainData @classmethod def annotations_type(cls) -> Type[MyDomainAnnotations]: return MyDomainAnnotations def additional_method(self): ... # Define the domain specifier reporting the various classes for domain instances class MyDomainSpecifier(DomainSpecifier): @classmethod def domain_name(cls) -> str: return \"my domain\" @classmethod def description(cls) -> str: return \"A detailed description of the domain\" @classmethod def data_type(cls) -> Type[MyDomainData]: return MyDomainData @classmethod def annotations_type(cls) -> Type[MyDomainAnnotations]: return MyDomainAnnotations @classmethod def instance_class(cls) -> Type[MyDomainInstance]: return MyDomainInstance","title":"Example"},{"location":"plugin/#stages","text":"Stages are a partial pipeline of components. Input stages consist of a source component and zero or more processing components, processing stages only consist of processing components, and output stages consist of zero or more processing components followed by a sink component. To create a new stage, the individual components must be created and then advertised through a stage specifier class. The base classes for components can be imported from wai-annotations' core package: from wai.annotations.core.component import SourceComponent from wai.annotations.core.component import ProcessorComponent from wai.annotations.core.component import SinkComponent Sub-class the base classes for the types of components you are trying to implement, and fill in the generic type-parameters and abstract methods. See the examples below for guidance. Then sub-class one of the stage-specifier classes for the type of stage you are creating: from wai.annotations.core.specifier import SourceStageSpecifier from wai.annotations.core.specifier import ProcessorStageSpecifier from wai.annotations.core.specifier import SinkStageSpecifier The specifier classes require that you override the description method, which provides a description of the stage, and the components method, which lists the components which comprise the stage. Source/Sink stages also require a domain method, which returns the domain that the stage reads/writes. Processor stages instead have a domain_transfer_function method, which returns the output domain for a given input domain (this way, processing stages can be used in multiple domains). In order for wai-annotations to recognise your plugin, the specifier class needs to be advertised as an entry point in your setup script under the wai.annotations.plugins group: # setup.py from setuptools import setup setup( ..., entry_points={ \"wai.annotations.plugins\": [ # Input stage \"from-my-input-format=com.example.specifiers:MySourceStageSpecifier\", # Output stage \"to-my-output-format=com.example.specifiers:MySinkStagepecifier\", # Processor stages \"my-processor=com.example.specifiers:MyProcessorStageSpecifier\" ] } )","title":"Stages"},{"location":"plugin/#example-input-stage","text":"Input stages consist of a source component followed by zero or more processing components. Typically the source component will be wai.annotations.core.component.util.LocalFilenameSource , but this is not required. What is required is that the final component of the stage outputs instances in the domain that the stage operates. from typing import Type, Tuple from wai.annotations.core.component import SourceComponent, ProcessorComponent from wai.annotations.domain.image.object_detection import ( ImageObjectDetectionInstance, ImageObjectDetectionDomainSpecifier ) from wai.annotations.core.specifier import SourceStageSpecifier from wai.annotations.core.stream import ThenFunction, DoneFunction # Define the source component (generic type is the type this source produces) class MySourceComponent(SourceComponent[str]): # Any command-line options here... def produce( self, then: ThenFunction[str], done: DoneFunction ): # Call then(str) multiple times... ... # Call done() done() # Define a processor component to parse each string from the source component into a domain instance # (generic types are input and output type respectively) class MyProcessorComponent(ProcessorComponent[str, ImageObjectDetectionInstance]): # Any command-line options here... def process_element( self, element: str, then: ThenFunction[ImageObjectDetectionInstance], done: DoneFunction ): # Parse each string and forward then(self._parse(element)) def finish( self, then: ThenFunction[ImageObjectDetectionInstance], done: DoneFunction ): # Perform any clean-up ... # Call done done() def _parse(self, element: str) -> ImageObjectDetectionInstance: # Parse the string into an instance ... # Create the specifier class for the stage class MySourceStageSpecifier(SourceStageSpecifier): @classmethod def description(cls) -> str: return \"My source stage\" @classmethod def components(cls) -> Tuple[Type[MySourceComponent], Type[MyProcessorComponent]]: return MySourceComponent, MyProcessorComponent @classmethod def domain(cls) -> Type[ImageObjectDetectionDomainSpecifier]: return ImageObjectDetectionDomainSpecifier","title":"Example Input Stage"},{"location":"plugin/#example-output-stage","text":"Output stages consist of zero or more processing components followed by a single sink component. The first component of the stage must take instances in the stage's domain as input. from typing import Type, Tuple from wai.annotations.core.component import ProcessorComponent, SinkComponent from wai.annotations.domain.image.object_detection import ( ImageObjectDetectionInstance, ImageObjectDetectionDomainSpecifier ) from wai.annotations.core.specifier import SinkStageSpecifier from wai.annotations.core.stream import ThenFunction, DoneFunction # Define a processor component to format each domain instance into a string # (generic types are input and output type respectively) class MyProcessorComponent(ProcessorComponent[ImageObjectDetectionInstance, str]): # Any command-line options here... def process_element( self, element: ImageObjectDetectionInstance, then: ThenFunction[str], done: DoneFunction ): # Format each instance and forward then(self._format(element)) def finish( self, then: ThenFunction[ImageObjectDetectionInstance], done: DoneFunction ): # Perform any clean-up ... # Call done done() def _format(self, element: ImageObjectDetectionInstance) -> str: # Format the instance into an str ... # Define the sink component (generic type is the type this sink consumes) class MySinkComponent(SinkComponent[str]): # Any command-line options here... def consume_element(self, element: str): # E.g. write the string to disk ... def finish(self): # Any tidy-up ... # Create the specifier class for the stage class MySinkStageSpecifier(SinkStageSpecifier): @classmethod def description(cls) -> str: return \"My source stage\" @classmethod def components(cls) -> Tuple[Type[MyProcessorComponent], Type[MySinkComponent]]: return MyProcessorComponent, MySinkComponent @classmethod def domain(cls) -> Type[ImageObjectDetectionDomainSpecifier]: return ImageObjectDetectionDomainSpecifier","title":"Example Output Stage"},{"location":"plugin/#example-processor-stage","text":"Processor stages consist of one or more processing components. The domain-transfer function defined on the specifier must match the input/output types of the chain of components for the stage. from typing import Type, Tuple from wai.annotations.core.component import ProcessorComponent from wai.annotations.core.domain import Instance, DomainSpecifier from wai.annotations.core.specifier import ProcessorStageSpecifier from wai.annotations.core.stream import ThenFunction, DoneFunction from wai.annotations.core.stream.util import ProcessState # Define a processor component to remove every second instance (for any domain) # (generic types are input and output type respectively) class MyProcessorComponent(ProcessorComponent[Instance, Instance]): # Any command-line options here... # Process-state to track whether we need to remove the next instance remove_next: bool = ProcessState(lambda self: False) def process_element( self, element: Instance, then: ThenFunction[Instance], done: DoneFunction ): if not self.remove_next: then(element) self.remove_next = not self.remove_next def finish( self, then: ThenFunction[Instance], done: DoneFunction ): # Perform any clean-up ... # Call done done() # Create the specifier class for the stage class MyProcessorStageSpecifier(ProcessorStageSpecifier): @classmethod def description(cls) -> str: return \"My source stage\" @classmethod def components(cls) -> Tuple[Type[MyProcessorComponent]]: return MyProcessorComponent, @classmethod def domain_transfer_function( cls, input_domain: Type[DomainSpecifier] ) -> Type[DomainSpecifier]: # Because our processor stage works in any domain, and does not cross domains, just return the input domain return input_domain","title":"Example Processor Stage"},{"location":"plugin/#best-practice","text":"Although in each of the examples shown here, we have defined our plugin specifiers in the same file as the components they advertise, this is not the recommended approach. The specifier types should instead be defined in their own sub-package, and the methods should locally import the specified types (instead of globally at the beginning of the specifier Python file). This is so the specifier can be imported into the plugin system without importing potentially heavy-weight libraries that the components depend on for their functionality. This way the system can provide reflection of the available plugins, but only load those plugins that are actually selected for use in a conversion.","title":"Best Practice"},{"location":"plugin/#adding-command-line-options-to-plugin-components","text":"See here for a description of command-line option support in wai-annotations.","title":"Adding Command-Line Options to Plugin Components"},{"location":"plugin/#splitting","text":"Sink-components (and by extension sink-stages) may require that the incoming instances be split across a number of output locations. Special support for splitting is provided in the wai.annotations.core.component.util sub-package. See here for information on how to add splitting to your sink-stages.","title":"Splitting"},{"location":"plugins/","text":"Plugins Source stage FROM-ADAMS-IC Reads image classification annotations in the ADAMS report-format Domain(s): Image Classification Domain Options: usage: from-adams-ic [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [-e FORMAT FORMAT FORMAT] -c FIELD optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) -e FORMAT FORMAT FORMAT, --extensions FORMAT FORMAT FORMAT image format extensions in order of preference (default: [<ImageFormat.PNG: (frozenset({'png', 'PNG'}), 'PNG')>, <ImageFormat.JPG: (frozenset({'jpeg', 'JPG', 'JPEG', 'jpg'}), 'JPEG')>, <ImageFormat.BMP: (frozenset({'bmp', 'BMP'}), 'BMP')>]) -c FIELD, --class-field FIELD the report field containing the image class (default: None) FROM-ADAMS-OD Reads image object-detection annotations in the ADAMS report-format Domain(s): Image Object-Detection Domain Options: usage: from-adams-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [-e FORMAT FORMAT FORMAT] [-p PREFIXES [PREFIXES ...]] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) -e FORMAT FORMAT FORMAT, --extensions FORMAT FORMAT FORMAT image format extensions in order of preference (default: [<ImageFormat.PNG: (frozenset({'png', 'PNG'}), 'PNG')>, <ImageFormat.JPG: (frozenset({'jpeg', 'JPG', 'JPEG', 'jpg'}), 'JPEG')>, <ImageFormat.BMP: (frozenset({'bmp', 'BMP'}), 'BMP')>]) -p PREFIXES [PREFIXES ...], --prefixes PREFIXES [PREFIXES ...] prefixes to parse (default: []) FROM-BLUE-CHANNEL-IS Reads image segmentation files in the blue-channel format Domain(s): Image Segmentation Domain Options: usage: from-blue-channel-is [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [--image-path-rel PATH] --labels LABEL [LABEL ...] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) --image-path-rel PATH Relative path to image files from annotations (default: .) --labels LABEL [LABEL ...] specifies the labels for each index (default: None) FROM-COCO-OD Reads image object-detection annotations in the MS-COCO JSON-format Domain(s): Image Object-Detection Domain Options: usage: from-coco-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) FROM-COMMON-VOICE-SP Reads speech transcriptions in the Mozilla Common-Voice TSV-format Domain(s): Speech Domain Options: usage: from-common-voice-sp [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [--rel-path REL_PATH] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) --rel-path REL_PATH the relative path from the annotations file to the audio files (default: .) FROM-FESTVOX-SP Reads speech transcriptions in the Festival FestVox format Domain(s): Speech Domain Options: usage: from-festvox-sp [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [--rel-path REL_PATH] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) --rel-path REL_PATH the relative path from the annotations file to the audio files (default: .) FROM-GRAYSCALE-IS Reads image segmentation files in the grayscale format Domain(s): Image Segmentation Domain Options: usage: from-grayscale-is [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [--image-path-rel PATH] --labels LABEL [LABEL ...] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) --image-path-rel PATH Relative path to image files from annotations (default: .) --labels LABEL [LABEL ...] specifies the labels for each index (default: None) FROM-IMAGES-IC Dummy reader that turns images into an image classification dataset. Domain(s): Image Classification Domain Options: usage: from-images-ic [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) FROM-IMAGES-IS Dummy reader that turns images into an image segmentation dataset. Domain(s): Image Segmentation Domain Options: usage: from-images-is [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) FROM-IMAGES-OD Dummy reader that turns images into an object detection dataset. Domain(s): Image Object-Detection Domain Options: usage: from-images-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) FROM-INDEXED-PNG-IS Reads image segmentation files in the indexed-PNG format Domain(s): Image Segmentation Domain Options: usage: from-indexed-png-is [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [--image-path-rel PATH] --labels LABEL [LABEL ...] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) --image-path-rel PATH Relative path to image files from annotations (default: .) --labels LABEL [LABEL ...] specifies the labels for each index (default: None) FROM-LAYER-SEGMENTS-IS Reads in the layer-segments image-segmentation format from disk, where each label has a binary PNG storing the mask for that label Domain(s): Image Segmentation Domain Options: usage: from-layer-segments-is [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [--label-separator SEPARATOR] --labels LABEL [LABEL ...] [--image-path-rel PATH] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) --label-separator SEPARATOR the separator between the base filename and the label (default: -) --labels LABEL [LABEL ...] specifies the labels for each index (default: None) --image-path-rel PATH Relative path to image files from annotations (default: .) FROM-OPEX-OD Reads image object-detection annotations in the OPEX format Domain(s): Image Object-Detection Domain Options: usage: from-opex-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) FROM-ROI-OD Reads image object-detection annotations in the ROI CSV-format Domain(s): Image Object-Detection Domain Options: usage: from-roi-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [-e FORMAT FORMAT FORMAT] [--prefix READER_PREFIX] [--suffix READER_SUFFIX] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) -e FORMAT FORMAT FORMAT, --extensions FORMAT FORMAT FORMAT image format extensions in order of preference (default: [<ImageFormat.PNG: (frozenset({'png', 'PNG'}), 'PNG')>, <ImageFormat.JPG: (frozenset({'jpeg', 'JPG', 'JPEG', 'jpg'}), 'JPEG')>, <ImageFormat.BMP: (frozenset({'bmp', 'BMP'}), 'BMP')>]) --prefix READER_PREFIX the prefix for output filenames (default = '') (default: None) --suffix READER_SUFFIX the suffix for output filenames (default = '-rois.csv') (default: None) FROM-SUBDIR-IC Reads images from sub-directories named after their class labels. Domain(s): Image Classification Domain Options: usage: from-subdir-ic [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) FROM-TF-OD Reads image object-detection annotations in the TFRecords binary format Domain(s): Image Object-Detection Domain Options: usage: from-tf-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [--mask-threshold THRESHOLD] [--sample-stride STRIDE] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) --mask-threshold THRESHOLD the threshold to use when calculating polygons from masks (default: 0.9) --sample-stride STRIDE the stride to use when calculating polygons from masks (default: 1) FROM-VGG-OD Reads image object-detection annotations in the VGG JSON-format Domain(s): Image Object-Detection Domain Options: usage: from-vgg-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) FROM-VIDEO-FILE-OD Reads frames from a video file. Domain(s): Image Object-Detection Domain Options: usage: from-video-file-od [-f FROM_FRAME] [-i INPUT_FILE] [-m MAX_FRAMES] [-n NTH_FRAME] [-p PREFIX] [-t TO_FRAME] optional arguments: -f FROM_FRAME, --from-frame FROM_FRAME determines with which frame to start the stream (1-based index) (default: 1) -i INPUT_FILE, --input INPUT_FILE the video file to read (default: ) -m MAX_FRAMES, --max-frames MAX_FRAMES determines the maximum number of frames to read; ignored if <=0 (default: -1) -n NTH_FRAME, --nth-frame NTH_FRAME determines whether frames get skipped and only evert nth frame gets forwarded (default: 1) -p PREFIX, --prefix PREFIX the prefix to use for the frames (default: ) -t TO_FRAME, --to-frame TO_FRAME determines after which frame to stop (1-based index); ignored if <=0 (default: -1) FROM-VOC-OD Reads image object-detection annotations in the Pascal VOC XML-format Domain(s): Image Object-Detection Domain Options: usage: from-voc-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) FROM-WEBCAM-OD Reads frames from a webcam. Domain(s): Image Object-Detection Domain Options: usage: from-webcam-od [-f FROM_FRAME] [-m MAX_FRAMES] [-n NTH_FRAME] [-p PREFIX] [-t TO_FRAME] [-i WEBCAM_ID] optional arguments: -f FROM_FRAME, --from-frame FROM_FRAME determines with which frame to start the stream (1-based index) (default: 1) -m MAX_FRAMES, --max-frames MAX_FRAMES determines the maximum number of frames to read; ignored if <=0 (default: -1) -n NTH_FRAME, --nth-frame NTH_FRAME determines whether frames get skipped and only evert nth frame gets forwarded (default: 1) -p PREFIX, --prefix PREFIX the prefix to use for the frames (default: webcam-) -t TO_FRAME, --to-frame TO_FRAME determines after which frame to stop (1-based index); ignored if <=0 (default: -1) -i WEBCAM_ID, --webcam-id WEBCAM_ID the webcam ID to read from (default: 0) FROM-YOLO-OD Reads image object-detection annotations in the YOLO format Domain(s): Image Object-Detection Domain Options: usage: from-yolo-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [--image-path-rel PATH] [-l PATH] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) --image-path-rel PATH Relative path to image files from annotations (default: None) -l PATH, --labels PATH Path to the labels file (default: None) Processor stage ADD-ANNOTATION-OVERLAY-IC Adds the image classification label on top of images passing through. Domain(s): Image Classification Domain Options: usage: add-annotation-overlay-ic [--background-color BACKGROUND_COLOR] [--background-margin BACKGROUND_MARGIN] [--fill-background] [--font-color FONT_COLOR] [--font-family FONT_FAMILY] [--font-size FONT_SIZE] [--position TEXT_PLACEMENT] optional arguments: --background-color BACKGROUND_COLOR the RGB color triplet to use for the background. (default: 0,0,0) --background-margin BACKGROUND_MARGIN the margin in pixels around the background. (default: 2) --fill-background whether to fill the background of the text with the specified color. (default: False) --font-color FONT_COLOR the RGB color triplet to use for the font. (default: 255,255,255) --font-family FONT_FAMILY the name of the TTF font-family to use, note: any hyphens need escaping with backslash. (default: sans\\-serif) --font-size FONT_SIZE the size of the font. (default: 14) --position TEXT_PLACEMENT the position of the label (X,Y). (default: 5,5) ADD-ANNOTATION-OVERLAY-IS Adds the image segmentation annotations on top of images passing through. Domain(s): Image Segmentation Domain Options: usage: add-annotation-overlay-is [--alpha ALPHA] [--colors COLORS [COLORS ...]] [--labels LABELS [LABELS ...]] optional arguments: --alpha ALPHA the alpha value to use for overlaying the annotations (0: transparent, 255: opaque). (default: 64) --colors COLORS [COLORS ...] the RGB triplets (R,G,B) of custom colors to use, uses default colors if not supplied (default: []) --labels LABELS [LABELS ...] the labels of annotations to overlay, overlays all if omitted (default: []) ADD-ANNOTATION-OVERLAY-OD Adds object detection overlays to images passing through. Domain(s): Image Object-Detection Domain Options: usage: add-annotation-overlay-od [--colors COLORS [COLORS ...]] [--fill] [--fill-alpha FILL_ALPHA] [--font-family FONT_FAMILY] [--font-size FONT_SIZE] [--force-bbox] [--label-key LABEL_KEY] [--labels LABELS [LABELS ...]] [--num-decimals NUM_DECIMALS] [--outline-alpha OUTLINE_ALPHA] [--outline-thickness OUTLINE_THICKNESS] [--text-format TEXT_FORMAT] [--text-placement TEXT_PLACEMENT] [--vary-colors] optional arguments: --colors COLORS [COLORS ...] the RGB triplets (R,G,B) of custom colors to use, uses default colors if not supplied (default: []) --fill whether to fill the bounding boxes/polygons (default: False) --fill-alpha FILL_ALPHA the alpha value to use for the filling (0: transparent, 255: opaque). (default: 128) --font-family FONT_FAMILY the name of the TTF font-family to use, note: any hyphens need escaping with backslash. (default: sans\\-serif) --font-size FONT_SIZE the size of the font. (default: 14) --force-bbox whether to force a bounding box even if there is a polygon available (default: False) --label-key LABEL_KEY the key in the meta-data that contains the label. (default: type) --labels LABELS [LABELS ...] the labels of annotations to overlay, overlays all if omitted (default: []) --num-decimals NUM_DECIMALS the number of decimals to use for float numbers in the text format string. (default: 3) --outline-alpha OUTLINE_ALPHA the alpha value to use for the outline (0: transparent, 255: opaque). (default: 255) --outline-thickness OUTLINE_THICKNESS the line thickness to use for the outline, <1 to turn off. (default: 3) --text-format TEXT_FORMAT template for the text to print on top of the bounding box or polygon, '{PH}' is a placeholder for the 'PH' value from the meta-data or 'label' for the current label; ignored if empty. (default: {label}) --text-placement TEXT_PLACEMENT comma-separated list of vertical (T=top, C=center, B=bottom) and horizontal (L=left, C=center, R=right) anchoring. (default: T,L) --vary-colors whether to vary the colors of the outline/filling regardless of label (default: False) CHECK-DUPLICATE-FILENAMES Causes the conversion stream to halt when multiple dataset items have the same filename Domain(s): Image Segmentation Domain Image Object-Detection Domain Speech Domain Image Classification Domain Options: usage: check-duplicate-filenames COERCE-BOX Converts all annotation bounds into box regions Domain(s): Image Object-Detection Domain Options: usage: coerce-box COERCE-MASK Converts all annotation bounds into polygon regions Domain(s): Image Object-Detection Domain Options: usage: coerce-mask COMBINE-ANNOTATIONS-OD Combines object detection annotations from images passing through into a single annotation. Domain(s): Image Object-Detection Domain Options: usage: combine-annotations-od [--combination COMBINATION] [--min-iou MIN_IOU] optional arguments: --combination COMBINATION how to combine the annotations (union|intersect); the 'stream_index' key in the meta-data contains the stream index (default: intersect) --min-iou MIN_IOU the minimum IoU (intersect over union) to use for identifying objects that overlap (default: 0.7) CONVERT-IMAGE-FORMAT Converts images from one format to another Domain(s): Image Segmentation Domain Image Object-Detection Domain Image Classification Domain Options: usage: convert-image-format -f FORMAT optional arguments: -f FORMAT, --format FORMAT format to convert images to (default: None) CROP Crops images. Domain(s): Image Object-Detection Domain Image Classification Domain Options: usage: crop [-m IMGAUG_MODE] [--suffix IMGAUG_SUFFIX] [-f PERCENT_FROM] [-t PERCENT_TO] [-s SEED] [-a] [-T THRESHOLD] [-u] optional arguments: -m IMGAUG_MODE, --mode IMGAUG_MODE the image augmentation mode to use, available modes: replace, add (default: replace) --suffix IMGAUG_SUFFIX the suffix to use for the file names in case of augmentation mode add (default: None) -f PERCENT_FROM, --from-percent PERCENT_FROM the minimum percent to crop from images (default: None) -t PERCENT_TO, --to-percent PERCENT_TO the maximum percent to crop from images (default: None) -s SEED, --seed SEED the seed value to use for the random number generator; randomly seeded if not provided (default: None) -a, --seed-augmentation whether to seed the augmentation; if specified, uses the seeded random generator to produce a seed value from 0 to 1000 for the augmentation. (default: False) -T THRESHOLD, --threshold THRESHOLD the threshold to use for Random.rand(): if equal or above, augmentation gets applied; range: 0-1; default: 0 (= always) (default: None) -u, --update-size whether to update the image size after the crop operation or scale back to original size (default: False) DIMENSION-DISCARDER Removes annotations which fall outside certain size constraints Domain(s): Image Object-Detection Domain Options: usage: dimension-discarder [--max-area MAX_AREA] [--max-height MAX_HEIGHT] [--max-width MAX_WIDTH] [--min-area MIN_AREA] [--min-height MIN_HEIGHT] [--min-width MIN_WIDTH] [--verbose] optional arguments: --max-area MAX_AREA the maximum area of annotations to convert (default: None) --max-height MAX_HEIGHT the maximum height of annotations to convert (default: None) --max-width MAX_WIDTH the maximum width of annotations to convert (default: None) --min-area MIN_AREA the minimum area of annotations to convert (default: None) --min-height MIN_HEIGHT the minimum height of annotations to convert (default: None) --min-width MIN_WIDTH the minimum width of annotations to convert (default: None) --verbose outputs information when discarding annotations (default: False) DISCARD-INVALID-IMAGES Discards images that cannot be loaded (e.g., corrupt image file or annotations with no image) Domain(s): Image Segmentation Domain Image Object-Detection Domain Image Classification Domain Options: usage: discard-invalid-images [-v] optional arguments: -v, --verbose whether to output debugging information (default: False) DISCARD-NEGATIVES Discards negative examples (those without annotations) from the stream Domain(s): Image Segmentation Domain Image Object-Detection Domain Speech Domain Image Classification Domain Options: usage: discard-negatives DROP-FRAMES Drops frames from the stream. Domain(s): Image Segmentation Domain Image Object-Detection Domain Image Classification Domain Options: usage: drop-frames [-n NTH_FRAME] optional arguments: -n NTH_FRAME, --nth-frame NTH_FRAME which nth frame to drop, e..g, '2' means to drop every 2nd frame; passes frames through if <=1 (default: 0) FILTER-FRAMES-BY-LABEL-OD Filters frames from the stream using the labels in the annotations, i.e., keeps or drops frames depending on presence/absence of labels. Domain(s): Image Segmentation Domain Image Object-Detection Domain Image Classification Domain Options: usage: filter-frames-by-label-od [--excluded-labels EXCLUDED_LABELS] [--key-label KEY_LABEL] [--key-score KEY_SCORE] [--min-score MIN_SCORE] [--required-labels REQUIRED_LABELS] [-v] optional arguments: --excluded-labels EXCLUDED_LABELS the comma-separated list of labels that will automatically drop the frame when present in the frame (default: ) --key-label KEY_LABEL the meta-data key in the annotations that contains the label. (default: type) --key-score KEY_SCORE the meta-data key in the annotations to use for storing the prediction score. (default: score) --min-score MIN_SCORE the minimum score that predictions must have in order to be included in the label checks, ignored if not supplied (default: None) --required-labels REQUIRED_LABELS the comma-separated list of labels that must be present in the frame, otherwise it gets dropped (default: ) -v, --verbose whether to output debugging information. (default: False) FILTER-LABELS Filters detected objects down to those with specified labels. Domain(s): Image Object-Detection Domain Options: usage: filter-labels [-l LABELS [LABELS ...]] [-r regexp] optional arguments: -l LABELS [LABELS ...], --labels LABELS [LABELS ...] labels to use (default: []) -r regexp, --regexp regexp regular expression for using only a subset of labels (default: None) FILTER-METADATA Filters detected objects based on their meta-data. Domain(s): Image Object-Detection Domain Options: usage: filter-metadata [-c COMPARISON] [-k KEY] [-t VALUE_TYPE] optional arguments: -c COMPARISON, --comparison COMPARISON the comparison to apply to the value: for bool/numeric/string '=OTHER' and '!=OTHER' can be used, for numeric furthermore '<OTHER', '<=OTHER', '>=OTHER', '>OTHER'. E.g.: '<3.0' for numeric types will discard any annotations that have a value of 3.0 or larger (default: None) -k KEY, --key KEY the key of the meta-data value to use for the filtering (default: None) -t VALUE_TYPE, --value-type VALUE_TYPE the data type that the value represents, available options: bool|numeric|string (default: None) FLIP Flips images either left-to-right, up-to-down or both. Domain(s): Image Object-Detection Domain Image Classification Domain Options: usage: flip [-d DIRECTION] [-m IMGAUG_MODE] [--suffix IMGAUG_SUFFIX] [-s SEED] [-a] [-T THRESHOLD] optional arguments: -d DIRECTION, --direction DIRECTION the direction to flip, available options: lr, up, lrup (default: None) -m IMGAUG_MODE, --mode IMGAUG_MODE the image augmentation mode to use, available modes: replace, add (default: replace) --suffix IMGAUG_SUFFIX the suffix to use for the file names in case of augmentation mode add (default: None) -s SEED, --seed SEED the seed value to use for the random number generator; randomly seeded if not provided (default: None) -a, --seed-augmentation whether to seed the augmentation; if specified, uses the seeded random generator to produce a seed value from 0 to 1000 for the augmentation. (default: False) -T THRESHOLD, --threshold THRESHOLD the threshold to use for Random.rand(): if equal or above, augmentation gets applied; range: 0-1; default: 0 (= always) (default: None) GAUSSIAN-BLUR Applies gaussian blur to images. Domain(s): Image Object-Detection Domain Image Classification Domain Options: usage: gaussian-blur [-m IMGAUG_MODE] [--suffix IMGAUG_SUFFIX] [-s SEED] [-a] [-f SIGMA_FROM] [-t SIGMA_TO] [-T THRESHOLD] optional arguments: -m IMGAUG_MODE, --mode IMGAUG_MODE the image augmentation mode to use, available modes: replace, add (default: replace) --suffix IMGAUG_SUFFIX the suffix to use for the file names in case of augmentation mode add (default: None) -s SEED, --seed SEED the seed value to use for the random number generator; randomly seeded if not provided (default: None) -a, --seed-augmentation whether to seed the augmentation; if specified, uses the seeded random generator to produce a seed value from 0 to 1000 for the augmentation. (default: False) -f SIGMA_FROM, --from-sigma SIGMA_FROM the minimum sigma for the blur to apply to the images (default: None) -t SIGMA_TO, --to-sigma SIGMA_TO the maximum sigma for the blur to apply to the images (default: None) -T THRESHOLD, --threshold THRESHOLD the threshold to use for Random.rand(): if equal or above, augmentation gets applied; range: 0-1; default: 0 (= always) (default: None) HSL-GRAYSCALE Turns RGB images into fake grayscale ones by converting them to HSL and then using the L channel for all channels. The brightness can be influenced and varied even. Domain(s): Image Object-Detection Domain Image Classification Domain Options: usage: hsl-grayscale [-f FACTOR_FROM] [-t FACTOR_TO] [-m IMGAUG_MODE] [--suffix IMGAUG_SUFFIX] [-s SEED] [-a] [-T THRESHOLD] optional arguments: -f FACTOR_FROM, --from-factor FACTOR_FROM the start of the factor range to apply to the L channel to darken or lighten the image (<1: darker, >1: lighter) (default: None) -t FACTOR_TO, --to-factor FACTOR_TO the end of the factor range to apply to the L channel to darken or lighten the image (<1: darker, >1: lighter) (default: None) -m IMGAUG_MODE, --mode IMGAUG_MODE the image augmentation mode to use, available modes: replace, add (default: replace) --suffix IMGAUG_SUFFIX the suffix to use for the file names in case of augmentation mode add (default: None) -s SEED, --seed SEED the seed value to use for the random number generator; randomly seeded if not provided (default: None) -a, --seed-augmentation whether to seed the augmentation; if specified, uses the seeded random generator to produce a seed value from 0 to 1000 for the augmentation. (default: False) -T THRESHOLD, --threshold THRESHOLD the threshold to use for Random.rand(): if equal or above, augmentation gets applied; range: 0-1; default: 0 (= always) (default: None) LINEAR-CONTRAST Applies linear contrast to images. Domain(s): Image Object-Detection Domain Image Classification Domain Options: usage: linear-contrast [-f ALPHA_FROM] [-t ALPHA_TO] [-m IMGAUG_MODE] [--suffix IMGAUG_SUFFIX] [-s SEED] [-a] [-T THRESHOLD] optional arguments: -f ALPHA_FROM, --from-alpha ALPHA_FROM the minimum alpha to apply to the images (default: None) -t ALPHA_TO, --to-alpha ALPHA_TO the maximum alpha to apply to the images (default: None) -m IMGAUG_MODE, --mode IMGAUG_MODE the image augmentation mode to use, available modes: replace, add (default: replace) --suffix IMGAUG_SUFFIX the suffix to use for the file names in case of augmentation mode add (default: None) -s SEED, --seed SEED the seed value to use for the random number generator; randomly seeded if not provided (default: None) -a, --seed-augmentation whether to seed the augmentation; if specified, uses the seeded random generator to produce a seed value from 0 to 1000 for the augmentation. (default: False) -T THRESHOLD, --threshold THRESHOLD the threshold to use for Random.rand(): if equal or above, augmentation gets applied; range: 0-1; default: 0 (= always) (default: None) MAP-LABELS Maps object-detection labels from one set to another Domain(s): Image Object-Detection Domain Options: usage: map-labels [-m old=new] optional arguments: -m old=new, --mapping old=new mapping for labels, for replacing one label string with another (eg when fixing/collapsing labels) (default: []) OD-TO-IC Converts image object-detection instances into image classification instances Domain(s): Image Object-Detection Domain Options: usage: od-to-ic [-m HANDLER] optional arguments: -m HANDLER, --multiplicity HANDLER how to handle instances with more than one located object (default: error) OD-TO-IS Converts image object-detection instances into image segmentation instances Domain(s): Image Object-Detection Domain Options: usage: od-to-is [--label-error] --labels LABEL [LABEL ...] optional arguments: --label-error whether to raise errors when an unspecified label is encountered (default is to ignore) (default: False) --labels LABEL [LABEL ...] specifies the labels for each index (default: None) PASSTHROUGH Dummy ISP which has no effect on the conversion stream Domain(s): Image Segmentation Domain Image Object-Detection Domain Speech Domain Image Classification Domain Options: usage: passthrough POLYGON-DISCARDER Removes annotations with polygons which fall outside certain point limit constraints Domain(s): Image Object-Detection Domain Options: usage: polygon-discarder [--max-points MAX_POINTS] [--min-points MIN_POINTS] [--verbose] optional arguments: --max-points MAX_POINTS the maximum number of points in the polygon (default: None) --min-points MIN_POINTS the minimum number of points in the polygon (default: None) --verbose outputs information when discarding annotations (default: False) REDIS-PREDICT-IC Makes image classification predictions via Redis backend, passing in an image and receiving JSON predictions back (at least one of 'label: probability'). Predictions example: {\"dog\": 0.9, \"cat\": 0.1} Domain(s): Image Classification Domain Options: usage: redis-predict-ic [--channel-in CHANNEL_IN] [--channel-out CHANNEL_OUT] [-d REDIS_DB] [-h REDIS_HOST] [-p REDIS_PORT] [-t TIMEOUT] [-v] optional arguments: --channel-in CHANNEL_IN the Redis channel on which to receive predictions. (default: predictions) --channel-out CHANNEL_OUT the Redis channel to send the images out (default: images) -d REDIS_DB, --redis-db REDIS_DB the database to use (default: 0) -h REDIS_HOST, --redis-host REDIS_HOST the Redis server to connect to (default: localhost) -p REDIS_PORT, --redis-port REDIS_PORT the port the Redis server is running on (default: 6379) -t TIMEOUT, --timeout TIMEOUT the timeout in seconds to wait for a prediction to arrive (default: 5.0) -v, --verbose whether to output debugging information. (default: False) REDIS-PREDICT-IS Makes image segmentation predictions via Redis backend, passing in an image and receiving an image with predicted segmentations. Domain(s): Image Segmentation Domain Options: usage: redis-predict-is [--channel-in CHANNEL_IN] [--channel-out CHANNEL_OUT] [--image-format IMAGE_FORMAT] --labels LABEL [LABEL ...] [-d REDIS_DB] [-h REDIS_HOST] [-p REDIS_PORT] [-t TIMEOUT] [-v] optional arguments: --channel-in CHANNEL_IN the Redis channel on which to receive predictions. (default: predictions) --channel-out CHANNEL_OUT the Redis channel to send the images out (default: images) --image-format IMAGE_FORMAT the format of the image that comes back as prediction: indexedpng,bluechannel,grayscale (default: indexedpng) --labels LABEL [LABEL ...] specifies the labels for each index (default: None) -d REDIS_DB, --redis-db REDIS_DB the database to use (default: 0) -h REDIS_HOST, --redis-host REDIS_HOST the Redis server to connect to (default: localhost) -p REDIS_PORT, --redis-port REDIS_PORT the port the Redis server is running on (default: 6379) -t TIMEOUT, --timeout TIMEOUT the timeout in seconds to wait for a prediction to arrive (default: 5.0) -v, --verbose whether to output debugging information. (default: False) REDIS-PREDICT-OD Makes object detection predictions via Redis backend, passing in an image and receiving OPEX predictions back: https://github.com/WaikatoLink2020/objdet-predictions-exchange-format Domain(s): Image Object-Detection Domain Options: usage: redis-predict-od [--channel-in CHANNEL_IN] [--channel-out CHANNEL_OUT] [--key-label KEY_LABEL] [--key-score KEY_SCORE] [-d REDIS_DB] [-h REDIS_HOST] [-p REDIS_PORT] [-t TIMEOUT] [-v] optional arguments: --channel-in CHANNEL_IN the Redis channel on which to receive predictions. (default: predictions) --channel-out CHANNEL_OUT the Redis channel to send the images out (default: images) --key-label KEY_LABEL the meta-data key in the annotations to use for storing the label. (default: type) --key-score KEY_SCORE the meta-data key in the annotations to use for storing the prediction score. (default: score) -d REDIS_DB, --redis-db REDIS_DB the database to use (default: 0) -h REDIS_HOST, --redis-host REDIS_HOST the Redis server to connect to (default: localhost) -p REDIS_PORT, --redis-port REDIS_PORT the port the Redis server is running on (default: 6379) -t TIMEOUT, --timeout TIMEOUT the timeout in seconds to wait for a prediction to arrive (default: 5.0) -v, --verbose whether to output debugging information. (default: False) REMOVE-CLASSES Removes classes from classification/image-segmentation instances Domain(s): Image Segmentation Domain Image Classification Domain Options: usage: remove-classes -c CLASS [CLASS ...] optional arguments: -c CLASS [CLASS ...], --classes CLASS [CLASS ...] the classes to remove (default: None) ROTATE Rotates images randomly within a range of degrees or by a specified degree. Specify seed value and force augmentation to be seeded to generate repeatable augmentations. Domain(s): Image Object-Detection Domain Image Classification Domain Options: usage: rotate [-f DEGREE_FROM] [-t DEGREE_TO] [-m IMGAUG_MODE] [--suffix IMGAUG_SUFFIX] [-s SEED] [-a] [-T THRESHOLD] optional arguments: -f DEGREE_FROM, --from-degree DEGREE_FROM the start of the degree range to use for rotating the images (default: None) -t DEGREE_TO, --to-degree DEGREE_TO the end of the degree range to use for rotating the images (default: None) -m IMGAUG_MODE, --mode IMGAUG_MODE the image augmentation mode to use, available modes: replace, add (default: replace) --suffix IMGAUG_SUFFIX the suffix to use for the file names in case of augmentation mode add (default: None) -s SEED, --seed SEED the seed value to use for the random number generator; randomly seeded if not provided (default: None) -a, --seed-augmentation whether to seed the augmentation; if specified, uses the seeded random generator to produce a seed value from 0 to 1000 for the augmentation. (default: False) -T THRESHOLD, --threshold THRESHOLD the threshold to use for Random.rand(): if equal or above, augmentation gets applied; range: 0-1; default: 0 (= always) (default: None) SCALE Scales images randomly within a range of percentages or by a specified percentage. Specify seed value and force augmentation to be seeded to generate repeatable augmentations. Domain(s): Image Object-Detection Domain Image Classification Domain Options: usage: scale [-m IMGAUG_MODE] [--suffix IMGAUG_SUFFIX] [-k] [-f PERCENTAGE_FROM] [-t PERCENTAGE_TO] [-s SEED] [-a] [-T THRESHOLD] [-u] optional arguments: -m IMGAUG_MODE, --mode IMGAUG_MODE the image augmentation mode to use, available modes: replace, add (default: replace) --suffix IMGAUG_SUFFIX the suffix to use for the file names in case of augmentation mode add (default: None) -k, --keep-aspect whether to keep the aspect ratio (default: False) -f PERCENTAGE_FROM, --from-percentage PERCENTAGE_FROM the start of the percentage range to use for scaling the images (default: None) -t PERCENTAGE_TO, --to-percentage PERCENTAGE_TO the end of the percentage range to use for scaling the images (default: None) -s SEED, --seed SEED the seed value to use for the random number generator; randomly seeded if not provided (default: None) -a, --seed-augmentation whether to seed the augmentation; if specified, uses the seeded random generator to produce a seed value from 0 to 1000 for the augmentation. (default: False) -T THRESHOLD, --threshold THRESHOLD the threshold to use for Random.rand(): if equal or above, augmentation gets applied; range: 0-1; default: 0 (= always) (default: None) -u, --update-size whether to update the image size after the scaling operation or use original size (default: False) SKIP-SIMILAR-FRAMES Skips frames in the stream that are deemed too similar. Domain(s): Image Segmentation Domain Image Object-Detection Domain Image Classification Domain Options: usage: skip-similar-frames [-b BW_THRESHOLD] [-t CHANGE_THRESHOLD] [-c CONVERSION] [-v] optional arguments: -b BW_THRESHOLD, --bw-threshold BW_THRESHOLD the threshold to use for converting a gray-scale like image to black and white (0-255) (default: 128) -t CHANGE_THRESHOLD, --change-threshold CHANGE_THRESHOLD the percentage of pixels that changed relative to size of image (0-1) (default: 0.01) -c CONVERSION, --conversion CONVERSION how to convert the BGR image to a single channel image (gray/r/g/b) (default: gray) -v, --verbose whether to output some debugging output. (default: False) STRIP-ANNOTATIONS ISP which removes annotations from instances Domain(s): Image Segmentation Domain Image Object-Detection Domain Speech Domain Image Classification Domain Options: usage: strip-annotations SUB-IMAGES Extracts sub-images (incl their annotations) from the images coming through, using the defined regions. Domain(s): Image Object-Detection Domain Image Classification Domain Options: usage: sub-images [-p] [-s REGION_SORTING] [-r REGIONS [REGIONS ...]] [-e] optional arguments: -p, --include-partial whether to include only annotations that fit fully into a region or also partial ones (default: False) -s REGION_SORTING, --region-sorting REGION_SORTING how to sort the supplied region definitions: none|x-then-y|y-then-x (default: none) -r REGIONS [REGIONS ...], --regions REGIONS [REGIONS ...] the regions (X,Y,WIDTH,HEIGHT) to crop and forward with their annotations (default: []) -e, --suppress-empty suppresses sub-images that have no annotations (object detection) (default: False) Sink stage AREA-HISTOGRAM-IS Generates histograms of the area (normalized or absolute) occupied by the annotations. Domain(s): Image Segmentation Domain Options: usage: area-histogram-is [-a ALL_LABEL] [-b] [--label-key LABEL_KEY] [-n] [--num-bins NUM_BINS] [-o OUTPUT_FILE] [-f OUTPUT_FORMAT] optional arguments: -a ALL_LABEL, --all-label ALL_LABEL the label to use for all the labels combined (default: ALL) -b, --force-bbox whether to use the bounding box even if a polygon is present (object detection domain only) (default: False) --label-key LABEL_KEY the key in the meta-data that contains the label. (default: type) -n, --normalized whether to use normalized areas (using the image size as base). (default: False) --num-bins NUM_BINS the number of bins to use for the histogram. (default: 20) -o OUTPUT_FILE, --output OUTPUT_FILE the file to write the histogram to; uses stdout if omitted (default: ) -f OUTPUT_FORMAT, --format OUTPUT_FORMAT the format to use for the output, available modes: csv, json (default: text) AREA-HISTOGRAM-OD Generates histograms of the area (normalized or absolute) occupied by the annotations. Domain(s): Image Object-Detection Domain Options: usage: area-histogram-od [-a ALL_LABEL] [-b] [--label-key LABEL_KEY] [-n] [--num-bins NUM_BINS] [-o OUTPUT_FILE] [-f OUTPUT_FORMAT] optional arguments: -a ALL_LABEL, --all-label ALL_LABEL the label to use for all the labels combined (default: ALL) -b, --force-bbox whether to use the bounding box even if a polygon is present (object detection domain only) (default: False) --label-key LABEL_KEY the key in the meta-data that contains the label. (default: type) -n, --normalized whether to use normalized areas (using the image size as base). (default: False) --num-bins NUM_BINS the number of bins to use for the histogram. (default: 20) -o OUTPUT_FILE, --output OUTPUT_FILE the file to write the histogram to; uses stdout if omitted (default: ) -f OUTPUT_FORMAT, --format OUTPUT_FORMAT the format to use for the output, available modes: csv, json (default: text) CALC-FRAME-CHANGES Calculates the changes between frames, which can be used with the skip-similar-frames ISP. Domain(s): Image Object-Detection Domain Options: usage: calc-frame-changes [-b BW_THRESHOLD] [-t CHANGE_THRESHOLD] [-c CONVERSION] [-B NUM_BINS] [-o OUTPUT_FILE] [-f OUTPUT_FORMAT] [-v] optional arguments: -b BW_THRESHOLD, --bw-threshold BW_THRESHOLD the threshold to use for converting a gray-scale like image to black and white (0-255) (default: 128) -t CHANGE_THRESHOLD, --change-threshold CHANGE_THRESHOLD the percentage of pixels that changed relative to size of image (0-1) (default: 0.01) -c CONVERSION, --conversion CONVERSION how to convert the BGR image to a single channel image (gray/r/g/b) (default: gray) -B NUM_BINS, --num-bins NUM_BINS the number of bins to use for the histogram (default: 20) -o OUTPUT_FILE, --output OUTPUT_FILE the file to write to statistics to, stdout if not provided (default: ) -f OUTPUT_FORMAT, --output-format OUTPUT_FORMAT how to output the statistics (text/csv/json) (default: text) -v, --verbose whether to output some debugging output. (default: False) IMAGE-VIEWER-IC Displays image classification images. Domain(s): Image Classification Domain Options: usage: image-viewer-ic [--delay DELAY] [--position POSITION] [--size SIZE] [--title TITLE] optional arguments: --delay DELAY the delay in milli-seconds between images, use 0 to wait for keypress, ignored if <0 (default: 500) --position POSITION the position of the window on screen (X,Y) (default: 0,0) --size SIZE the maximum size for the image: WIDTH,HEIGHT (default: 640,480) --title TITLE the title for the window (default: wai.annotations) IMAGE-VIEWER-IS Displays image segmentation images. Domain(s): Image Segmentation Domain Options: usage: image-viewer-is [--delay DELAY] [--position POSITION] [--size SIZE] [--title TITLE] optional arguments: --delay DELAY the delay in milli-seconds between images, use 0 to wait for keypress, ignored if <0 (default: 500) --position POSITION the position of the window on screen (X,Y) (default: 0,0) --size SIZE the maximum size for the image: WIDTH,HEIGHT (default: 640,480) --title TITLE the title for the window (default: wai.annotations) IMAGE-VIEWER-OD Displays object detection images. Domain(s): Image Object-Detection Domain Options: usage: image-viewer-od [--delay DELAY] [--position POSITION] [--size SIZE] [--title TITLE] optional arguments: --delay DELAY the delay in milli-seconds between images, use 0 to wait for keypress, ignored if <0 (default: 500) --position POSITION the position of the window on screen (X,Y) (default: 0,0) --size SIZE the maximum size for the image: WIDTH,HEIGHT (default: 640,480) --title TITLE the title for the window (default: wai.annotations) LABEL-DIST-IC Generates a label distribution. Domain(s): Image Classification Domain Options: usage: label-dist-ic [--label-key LABEL_KEY] [-o OUTPUT_FILE] [-f OUTPUT_FORMAT] [-p] optional arguments: --label-key LABEL_KEY the key in the meta-data that contains the label. (default: type) -o OUTPUT_FILE, --output OUTPUT_FILE the file to write the statistics to; uses stdout if omitted (default: ) -f OUTPUT_FORMAT, --format OUTPUT_FORMAT the format to use for the output, available modes: csv, json (default: text) -p, --percentages whether to output percentages instead of counts. (default: False) LABEL-DIST-IS Generates a label distribution. Domain(s): Image Segmentation Domain Options: usage: label-dist-is [--label-key LABEL_KEY] [-o OUTPUT_FILE] [-f OUTPUT_FORMAT] [-p] optional arguments: --label-key LABEL_KEY the key in the meta-data that contains the label. (default: type) -o OUTPUT_FILE, --output OUTPUT_FILE the file to write the statistics to; uses stdout if omitted (default: ) -f OUTPUT_FORMAT, --format OUTPUT_FORMAT the format to use for the output, available modes: csv, json (default: text) -p, --percentages whether to output percentages instead of counts. (default: False) LABEL-DIST-OD Generates a label distribution. Domain(s): Image Object-Detection Domain Options: usage: label-dist-od [--label-key LABEL_KEY] [-o OUTPUT_FILE] [-f OUTPUT_FORMAT] [-p] optional arguments: --label-key LABEL_KEY the key in the meta-data that contains the label. (default: type) -o OUTPUT_FILE, --output OUTPUT_FILE the file to write the statistics to; uses stdout if omitted (default: ) -f OUTPUT_FORMAT, --format OUTPUT_FORMAT the format to use for the output, available modes: csv, json (default: text) -p, --percentages whether to output percentages instead of counts. (default: False) TO-ADAMS-IC Writes image classification annotations in the ADAMS report-format Domain(s): Image Classification Domain Options: usage: to-adams-ic -c FIELD [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: -c FIELD, --class-field FIELD the report field containing the image class (default: None) --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH output directory to write files to (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: []) TO-ADAMS-OD Writes image object-detection annotations in the ADAMS report-format Domain(s): Image Object-Detection Domain Options: usage: to-adams-od [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH output directory to write files to (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: []) TO-ANNOTATION-OVERLAY-OD Generates an image with all the annotation shapes (bbox or polygon) overlayed. Domain(s): Image Object-Detection Domain Options: usage: to-annotation-overlay-od [-b BACKGROUND_COLOR] [-c COLOR] [-o OUTPUT_FILE] [-s SCALE_TO] optional arguments: -b BACKGROUND_COLOR, --background-color BACKGROUND_COLOR the color to use for the background as RGBA byte-quadruplet, e.g.: 255,255,255,255 (default: 255,255,255,255) -c COLOR, --color COLOR the color to use for drawing the shapes as RGBA byte-quadruplet, e.g.: 255,0,0,64 (default: 255,0,0,64) -o OUTPUT_FILE, --output OUTPUT_FILE the PNG image to write the generated overlay to (default: ./overlay.png) -s SCALE_TO, --scale-to SCALE_TO the dimensions to scale all images to before overlaying them (format: width,height) (default: ) TO-BLUE-CHANNEL-IS Writes image segmentation files in the blue-channel format Domain(s): Image Segmentation Domain Options: usage: to-blue-channel-is [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH the directory to write the annotation images to (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: []) TO-COCO-OD Writes image object-detection annotations in the MS-COCO JSON-format Domain(s): Image Object-Detection Domain Options: usage: to-coco-od [--annotations-only] [--categories CATEGORY [CATEGORY ...]] [--category-output-file FILENAME] [--default-supercategory SUPERCATEGORY] [--error-on-new-category] [--license-name LICENSE_NAME] [--license-url LICENSE_URL] -o PATH [--pretty] [--sort-categories] [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) --categories CATEGORY [CATEGORY ...] defines the order of the categories (default: []) --category-output-file FILENAME file to write the categories into, as a simple comma-separated list (default: None) --default-supercategory SUPERCATEGORY the supercategory to use for pre-defined categories (default: Object) --error-on-new-category whether unspecified categories should raise an error (default: False) --license-name LICENSE_NAME the license of the images (default: default) --license-url LICENSE_URL the license of the images (default: ) -o PATH, --output PATH output file to write annotations to (images are placed in same directory) (default: None) --pretty whether to format the JSON annotations file with indentation (default: False) --sort-categories whether to put the categories in alphabetical order (default: False) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: []) TO-COMMON-VOICE-SP Writes speech transcriptions in the Mozilla Common-Voice TSV-format Domain(s): Speech Domain Options: usage: to-common-voice-sp [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH the filename of the TSV file to write the annotations into (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: []) TO-FESTVOX-SP Writes speech transcriptions in the Festival FestVox format Domain(s): Speech Domain Options: usage: to-festvox-sp [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH the filename of the FestVox file to write the annotations into (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: []) TO-GRAYSCALE-IS Writes image segmentation files in the grayscale format Domain(s): Image Segmentation Domain Options: usage: to-grayscale-is [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH the directory to write the annotation images to (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: []) TO-IMAGES-IC Dummy writer that just outputs images from image classification datasets. Domain(s): Image Classification Domain Options: usage: to-images-ic [-o OUTPUT_DIR] optional arguments: -o OUTPUT_DIR, --output-dir OUTPUT_DIR the directory to write the images to (default: .) TO-IMAGES-IS Dummy writer that just outputs images from image segmentation datasets. Domain(s): Image Segmentation Domain Options: usage: to-images-is [-o OUTPUT_DIR] optional arguments: -o OUTPUT_DIR, --output-dir OUTPUT_DIR the directory to write the images to (default: .) TO-IMAGES-OD Dummy writer that just outputs images from object detection datasets. Domain(s): Image Object-Detection Domain Options: usage: to-images-od [-o OUTPUT_DIR] optional arguments: -o OUTPUT_DIR, --output-dir OUTPUT_DIR the directory to write the images to (default: .) TO-INDEXED-PNG-IS Writes image segmentation files in the indexed-PNG format Domain(s): Image Segmentation Domain Options: usage: to-indexed-png-is [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH the directory to write the annotation images to (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: []) TO-LAYER-SEGMENTS-IS Writes the layer-segments image-segmentation format to disk Domain(s): Image Segmentation Domain Options: usage: to-layer-segments-is [--annotations-only] [--label-separator SEPARATOR] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) --label-separator SEPARATOR the separator between the base filename and the label (default: -) -o PATH, --output PATH the directory to write the annotation images to (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: []) TO-OPEX-OD Writes image object-detection annotations in the OPEX format Domain(s): Image Object-Detection Domain Options: usage: to-opex-od [-c PATH] [-l PATH] [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: -c PATH, --labels-csv PATH Path to the labels CSV file to write (default: None) -l PATH, --labels PATH Path to the labels file to write (default: None) --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH output directory to write images and annotations to (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: []) TO-ROI-OD Writes image object-detection annotations in the ROI CSV-format Domain(s): Image Object-Detection Domain Options: usage: to-roi-od [-d WIDTH HEIGHT] [--annotations-only] [--comments COMMENTS [COMMENTS ...]] -o PATH [--size-mode] [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] [--prefix WRITER_PREFIX] [--suffix WRITER_SUFFIX] optional arguments: -d WIDTH HEIGHT, --image-dimensions WIDTH HEIGHT image dimensions to use if none can be inferred (default: []) --annotations-only skip the writing of data files, outputting only the annotation files (default: False) --comments COMMENTS [COMMENTS ...] comments to write to the beginning of the ROI file (default: []) -o PATH, --output PATH output directory to write files to (default: None) --size-mode writes the ROI files with x,y,w,h headers instead of x0,y0,x1,y1 (default: False) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: []) --prefix WRITER_PREFIX the prefix for output filenames (default = '') (default: None) --suffix WRITER_SUFFIX the suffix for output filenames (default = '-rois.csv') (default: None) TO-SUBDIR-IC Writes images to sub-directories named after their class labels. Domain(s): Image Classification Domain Options: usage: to-subdir-ic -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: -o PATH, --output PATH the directory to store the class directories in (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: []) TO-TF-OD Writes image object-detection annotations in the TFRecords binary format Domain(s): Image Object-Detection Domain Options: usage: to-tf-od [--dense] [--source-id-type {filename,numeric-dummy}] -o PATH [-p FILENAME] [-s FILENAME [FILENAME ...]] [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --dense outputs masks in the dense numerical format instead of PNG-encoded (default: False) --source-id-type {filename,numeric-dummy} by default, the filename gets stored in the 'source_id' field, but some algorithms try to convert it into a number and fail with 'StringToNumberOp could not correctly convert string'; in which case you can use 'numeric- dummy' (see https://github.com/google/automl/issues/307) (default: filename) -o PATH, --output PATH name of output file for TFRecords (default: None) -p FILENAME, --protobuf FILENAME for storing the label strings and IDs (default: None) -s FILENAME [FILENAME ...], --shards FILENAME [FILENAME ...] additional shards to write to (default: []) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: []) TO-VGG-OD Writes image object-detection annotations in the VGG JSON-format Domain(s): Image Object-Detection Domain Options: usage: to-vgg-od [--annotations-only] -o PATH [--pretty] [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH output file to write annotations to (images are placed in same directory) (default: None) --pretty whether to format the JSON annotations file with indentation (default: False) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: []) TO-VIDEO-FILE-OD Writes frames to a MJPG video file. Domain(s): Image Object-Detection Domain Options: usage: to-video-file-od [-f FPS] [-o OUTPUT_FILE] optional arguments: -f FPS, --fps FPS the frames per second to use (default: 25) -o OUTPUT_FILE, --output OUTPUT_FILE the MJPG video file to write to (default: ) TO-VOC-OD Writes image object-detection annotations in the Pascal VOC XML-format Domain(s): Image Object-Detection Domain Options: usage: to-voc-od [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH output directory to write annotations to (images are placed in same directory) (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: []) TO-VOID-IC Consumes instances without writing them. Domain(s): Image Classification Domain Options: usage: to-void-ic TO-VOID-IS Consumes instances without writing them. Domain(s): Image Segmentation Domain Options: usage: to-void-is TO-VOID-OD Consumes instances without writing them. Domain(s): Image Object-Detection Domain Options: usage: to-void-od TO-VOID-SP Consumes instances without writing them. Domain(s): Speech Domain Options: usage: to-void-sp TO-YOLO-OD Writes image object-detection annotations in the YOLO format Domain(s): Image Object-Detection Domain Options: usage: to-yolo-od [-c PATH] [-l PATH] [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: -c PATH, --labels-csv PATH Path to the labels CSV file to write (default: None) -l PATH, --labels PATH Path to the labels file to write (default: None) --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH output directory to write images and annotations to (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: [])","title":"Plugins"},{"location":"plugins/#plugins","text":"","title":"Plugins"},{"location":"plugins/#source-stage","text":"","title":"Source stage"},{"location":"plugins/#from-adams-ic","text":"Reads image classification annotations in the ADAMS report-format","title":"FROM-ADAMS-IC"},{"location":"plugins/#domains","text":"Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options","text":"usage: from-adams-ic [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [-e FORMAT FORMAT FORMAT] -c FIELD optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) -e FORMAT FORMAT FORMAT, --extensions FORMAT FORMAT FORMAT image format extensions in order of preference (default: [<ImageFormat.PNG: (frozenset({'png', 'PNG'}), 'PNG')>, <ImageFormat.JPG: (frozenset({'jpeg', 'JPG', 'JPEG', 'jpg'}), 'JPEG')>, <ImageFormat.BMP: (frozenset({'bmp', 'BMP'}), 'BMP')>]) -c FIELD, --class-field FIELD the report field containing the image class (default: None)","title":"Options:"},{"location":"plugins/#from-adams-od","text":"Reads image object-detection annotations in the ADAMS report-format","title":"FROM-ADAMS-OD"},{"location":"plugins/#domains_1","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_1","text":"usage: from-adams-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [-e FORMAT FORMAT FORMAT] [-p PREFIXES [PREFIXES ...]] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) -e FORMAT FORMAT FORMAT, --extensions FORMAT FORMAT FORMAT image format extensions in order of preference (default: [<ImageFormat.PNG: (frozenset({'png', 'PNG'}), 'PNG')>, <ImageFormat.JPG: (frozenset({'jpeg', 'JPG', 'JPEG', 'jpg'}), 'JPEG')>, <ImageFormat.BMP: (frozenset({'bmp', 'BMP'}), 'BMP')>]) -p PREFIXES [PREFIXES ...], --prefixes PREFIXES [PREFIXES ...] prefixes to parse (default: [])","title":"Options:"},{"location":"plugins/#from-blue-channel-is","text":"Reads image segmentation files in the blue-channel format","title":"FROM-BLUE-CHANNEL-IS"},{"location":"plugins/#domains_2","text":"Image Segmentation Domain","title":"Domain(s):"},{"location":"plugins/#options_2","text":"usage: from-blue-channel-is [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [--image-path-rel PATH] --labels LABEL [LABEL ...] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) --image-path-rel PATH Relative path to image files from annotations (default: .) --labels LABEL [LABEL ...] specifies the labels for each index (default: None)","title":"Options:"},{"location":"plugins/#from-coco-od","text":"Reads image object-detection annotations in the MS-COCO JSON-format","title":"FROM-COCO-OD"},{"location":"plugins/#domains_3","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_3","text":"usage: from-coco-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None)","title":"Options:"},{"location":"plugins/#from-common-voice-sp","text":"Reads speech transcriptions in the Mozilla Common-Voice TSV-format","title":"FROM-COMMON-VOICE-SP"},{"location":"plugins/#domains_4","text":"Speech Domain","title":"Domain(s):"},{"location":"plugins/#options_4","text":"usage: from-common-voice-sp [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [--rel-path REL_PATH] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) --rel-path REL_PATH the relative path from the annotations file to the audio files (default: .)","title":"Options:"},{"location":"plugins/#from-festvox-sp","text":"Reads speech transcriptions in the Festival FestVox format","title":"FROM-FESTVOX-SP"},{"location":"plugins/#domains_5","text":"Speech Domain","title":"Domain(s):"},{"location":"plugins/#options_5","text":"usage: from-festvox-sp [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [--rel-path REL_PATH] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) --rel-path REL_PATH the relative path from the annotations file to the audio files (default: .)","title":"Options:"},{"location":"plugins/#from-grayscale-is","text":"Reads image segmentation files in the grayscale format","title":"FROM-GRAYSCALE-IS"},{"location":"plugins/#domains_6","text":"Image Segmentation Domain","title":"Domain(s):"},{"location":"plugins/#options_6","text":"usage: from-grayscale-is [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [--image-path-rel PATH] --labels LABEL [LABEL ...] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) --image-path-rel PATH Relative path to image files from annotations (default: .) --labels LABEL [LABEL ...] specifies the labels for each index (default: None)","title":"Options:"},{"location":"plugins/#from-images-ic","text":"Dummy reader that turns images into an image classification dataset.","title":"FROM-IMAGES-IC"},{"location":"plugins/#domains_7","text":"Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_7","text":"usage: from-images-ic [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None)","title":"Options:"},{"location":"plugins/#from-images-is","text":"Dummy reader that turns images into an image segmentation dataset.","title":"FROM-IMAGES-IS"},{"location":"plugins/#domains_8","text":"Image Segmentation Domain","title":"Domain(s):"},{"location":"plugins/#options_8","text":"usage: from-images-is [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None)","title":"Options:"},{"location":"plugins/#from-images-od","text":"Dummy reader that turns images into an object detection dataset.","title":"FROM-IMAGES-OD"},{"location":"plugins/#domains_9","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_9","text":"usage: from-images-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None)","title":"Options:"},{"location":"plugins/#from-indexed-png-is","text":"Reads image segmentation files in the indexed-PNG format","title":"FROM-INDEXED-PNG-IS"},{"location":"plugins/#domains_10","text":"Image Segmentation Domain","title":"Domain(s):"},{"location":"plugins/#options_10","text":"usage: from-indexed-png-is [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [--image-path-rel PATH] --labels LABEL [LABEL ...] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) --image-path-rel PATH Relative path to image files from annotations (default: .) --labels LABEL [LABEL ...] specifies the labels for each index (default: None)","title":"Options:"},{"location":"plugins/#from-layer-segments-is","text":"Reads in the layer-segments image-segmentation format from disk, where each label has a binary PNG storing the mask for that label","title":"FROM-LAYER-SEGMENTS-IS"},{"location":"plugins/#domains_11","text":"Image Segmentation Domain","title":"Domain(s):"},{"location":"plugins/#options_11","text":"usage: from-layer-segments-is [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [--label-separator SEPARATOR] --labels LABEL [LABEL ...] [--image-path-rel PATH] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) --label-separator SEPARATOR the separator between the base filename and the label (default: -) --labels LABEL [LABEL ...] specifies the labels for each index (default: None) --image-path-rel PATH Relative path to image files from annotations (default: .)","title":"Options:"},{"location":"plugins/#from-opex-od","text":"Reads image object-detection annotations in the OPEX format","title":"FROM-OPEX-OD"},{"location":"plugins/#domains_12","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_12","text":"usage: from-opex-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None)","title":"Options:"},{"location":"plugins/#from-roi-od","text":"Reads image object-detection annotations in the ROI CSV-format","title":"FROM-ROI-OD"},{"location":"plugins/#domains_13","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_13","text":"usage: from-roi-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [-e FORMAT FORMAT FORMAT] [--prefix READER_PREFIX] [--suffix READER_SUFFIX] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) -e FORMAT FORMAT FORMAT, --extensions FORMAT FORMAT FORMAT image format extensions in order of preference (default: [<ImageFormat.PNG: (frozenset({'png', 'PNG'}), 'PNG')>, <ImageFormat.JPG: (frozenset({'jpeg', 'JPG', 'JPEG', 'jpg'}), 'JPEG')>, <ImageFormat.BMP: (frozenset({'bmp', 'BMP'}), 'BMP')>]) --prefix READER_PREFIX the prefix for output filenames (default = '') (default: None) --suffix READER_SUFFIX the suffix for output filenames (default = '-rois.csv') (default: None)","title":"Options:"},{"location":"plugins/#from-subdir-ic","text":"Reads images from sub-directories named after their class labels.","title":"FROM-SUBDIR-IC"},{"location":"plugins/#domains_14","text":"Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_14","text":"usage: from-subdir-ic [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None)","title":"Options:"},{"location":"plugins/#from-tf-od","text":"Reads image object-detection annotations in the TFRecords binary format","title":"FROM-TF-OD"},{"location":"plugins/#domains_15","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_15","text":"usage: from-tf-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [--mask-threshold THRESHOLD] [--sample-stride STRIDE] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) --mask-threshold THRESHOLD the threshold to use when calculating polygons from masks (default: 0.9) --sample-stride STRIDE the stride to use when calculating polygons from masks (default: 1)","title":"Options:"},{"location":"plugins/#from-vgg-od","text":"Reads image object-detection annotations in the VGG JSON-format","title":"FROM-VGG-OD"},{"location":"plugins/#domains_16","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_16","text":"usage: from-vgg-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None)","title":"Options:"},{"location":"plugins/#from-video-file-od","text":"Reads frames from a video file.","title":"FROM-VIDEO-FILE-OD"},{"location":"plugins/#domains_17","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_17","text":"usage: from-video-file-od [-f FROM_FRAME] [-i INPUT_FILE] [-m MAX_FRAMES] [-n NTH_FRAME] [-p PREFIX] [-t TO_FRAME] optional arguments: -f FROM_FRAME, --from-frame FROM_FRAME determines with which frame to start the stream (1-based index) (default: 1) -i INPUT_FILE, --input INPUT_FILE the video file to read (default: ) -m MAX_FRAMES, --max-frames MAX_FRAMES determines the maximum number of frames to read; ignored if <=0 (default: -1) -n NTH_FRAME, --nth-frame NTH_FRAME determines whether frames get skipped and only evert nth frame gets forwarded (default: 1) -p PREFIX, --prefix PREFIX the prefix to use for the frames (default: ) -t TO_FRAME, --to-frame TO_FRAME determines after which frame to stop (1-based index); ignored if <=0 (default: -1)","title":"Options:"},{"location":"plugins/#from-voc-od","text":"Reads image object-detection annotations in the Pascal VOC XML-format","title":"FROM-VOC-OD"},{"location":"plugins/#domains_18","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_18","text":"usage: from-voc-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None)","title":"Options:"},{"location":"plugins/#from-webcam-od","text":"Reads frames from a webcam.","title":"FROM-WEBCAM-OD"},{"location":"plugins/#domains_19","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_19","text":"usage: from-webcam-od [-f FROM_FRAME] [-m MAX_FRAMES] [-n NTH_FRAME] [-p PREFIX] [-t TO_FRAME] [-i WEBCAM_ID] optional arguments: -f FROM_FRAME, --from-frame FROM_FRAME determines with which frame to start the stream (1-based index) (default: 1) -m MAX_FRAMES, --max-frames MAX_FRAMES determines the maximum number of frames to read; ignored if <=0 (default: -1) -n NTH_FRAME, --nth-frame NTH_FRAME determines whether frames get skipped and only evert nth frame gets forwarded (default: 1) -p PREFIX, --prefix PREFIX the prefix to use for the frames (default: webcam-) -t TO_FRAME, --to-frame TO_FRAME determines after which frame to stop (1-based index); ignored if <=0 (default: -1) -i WEBCAM_ID, --webcam-id WEBCAM_ID the webcam ID to read from (default: 0)","title":"Options:"},{"location":"plugins/#from-yolo-od","text":"Reads image object-detection annotations in the YOLO format","title":"FROM-YOLO-OD"},{"location":"plugins/#domains_20","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_20","text":"usage: from-yolo-od [-I FILENAME] [-i FILENAME] [-N FILENAME] [-n FILENAME] [-o FILENAME] [--seed SEED] [--image-path-rel PATH] [-l PATH] optional arguments: -I FILENAME, --inputs-file FILENAME Files containing lists of input files (can use glob syntax) (default: []) -i FILENAME, --input FILENAME Input files (can use glob syntax) (default: []) -N FILENAME, --negatives-file FILENAME Files containing lists of negative files (can use glob syntax) (default: []) -n FILENAME, --negative FILENAME Files that have no annotations (can use glob syntax) (default: []) -o FILENAME, --output-file FILENAME optional file to write read filenames into (default: None) --seed SEED the seed to use for randomisation (default: None) --image-path-rel PATH Relative path to image files from annotations (default: None) -l PATH, --labels PATH Path to the labels file (default: None)","title":"Options:"},{"location":"plugins/#processor-stage","text":"","title":"Processor stage"},{"location":"plugins/#add-annotation-overlay-ic","text":"Adds the image classification label on top of images passing through.","title":"ADD-ANNOTATION-OVERLAY-IC"},{"location":"plugins/#domains_21","text":"Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_21","text":"usage: add-annotation-overlay-ic [--background-color BACKGROUND_COLOR] [--background-margin BACKGROUND_MARGIN] [--fill-background] [--font-color FONT_COLOR] [--font-family FONT_FAMILY] [--font-size FONT_SIZE] [--position TEXT_PLACEMENT] optional arguments: --background-color BACKGROUND_COLOR the RGB color triplet to use for the background. (default: 0,0,0) --background-margin BACKGROUND_MARGIN the margin in pixels around the background. (default: 2) --fill-background whether to fill the background of the text with the specified color. (default: False) --font-color FONT_COLOR the RGB color triplet to use for the font. (default: 255,255,255) --font-family FONT_FAMILY the name of the TTF font-family to use, note: any hyphens need escaping with backslash. (default: sans\\-serif) --font-size FONT_SIZE the size of the font. (default: 14) --position TEXT_PLACEMENT the position of the label (X,Y). (default: 5,5)","title":"Options:"},{"location":"plugins/#add-annotation-overlay-is","text":"Adds the image segmentation annotations on top of images passing through.","title":"ADD-ANNOTATION-OVERLAY-IS"},{"location":"plugins/#domains_22","text":"Image Segmentation Domain","title":"Domain(s):"},{"location":"plugins/#options_22","text":"usage: add-annotation-overlay-is [--alpha ALPHA] [--colors COLORS [COLORS ...]] [--labels LABELS [LABELS ...]] optional arguments: --alpha ALPHA the alpha value to use for overlaying the annotations (0: transparent, 255: opaque). (default: 64) --colors COLORS [COLORS ...] the RGB triplets (R,G,B) of custom colors to use, uses default colors if not supplied (default: []) --labels LABELS [LABELS ...] the labels of annotations to overlay, overlays all if omitted (default: [])","title":"Options:"},{"location":"plugins/#add-annotation-overlay-od","text":"Adds object detection overlays to images passing through.","title":"ADD-ANNOTATION-OVERLAY-OD"},{"location":"plugins/#domains_23","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_23","text":"usage: add-annotation-overlay-od [--colors COLORS [COLORS ...]] [--fill] [--fill-alpha FILL_ALPHA] [--font-family FONT_FAMILY] [--font-size FONT_SIZE] [--force-bbox] [--label-key LABEL_KEY] [--labels LABELS [LABELS ...]] [--num-decimals NUM_DECIMALS] [--outline-alpha OUTLINE_ALPHA] [--outline-thickness OUTLINE_THICKNESS] [--text-format TEXT_FORMAT] [--text-placement TEXT_PLACEMENT] [--vary-colors] optional arguments: --colors COLORS [COLORS ...] the RGB triplets (R,G,B) of custom colors to use, uses default colors if not supplied (default: []) --fill whether to fill the bounding boxes/polygons (default: False) --fill-alpha FILL_ALPHA the alpha value to use for the filling (0: transparent, 255: opaque). (default: 128) --font-family FONT_FAMILY the name of the TTF font-family to use, note: any hyphens need escaping with backslash. (default: sans\\-serif) --font-size FONT_SIZE the size of the font. (default: 14) --force-bbox whether to force a bounding box even if there is a polygon available (default: False) --label-key LABEL_KEY the key in the meta-data that contains the label. (default: type) --labels LABELS [LABELS ...] the labels of annotations to overlay, overlays all if omitted (default: []) --num-decimals NUM_DECIMALS the number of decimals to use for float numbers in the text format string. (default: 3) --outline-alpha OUTLINE_ALPHA the alpha value to use for the outline (0: transparent, 255: opaque). (default: 255) --outline-thickness OUTLINE_THICKNESS the line thickness to use for the outline, <1 to turn off. (default: 3) --text-format TEXT_FORMAT template for the text to print on top of the bounding box or polygon, '{PH}' is a placeholder for the 'PH' value from the meta-data or 'label' for the current label; ignored if empty. (default: {label}) --text-placement TEXT_PLACEMENT comma-separated list of vertical (T=top, C=center, B=bottom) and horizontal (L=left, C=center, R=right) anchoring. (default: T,L) --vary-colors whether to vary the colors of the outline/filling regardless of label (default: False)","title":"Options:"},{"location":"plugins/#check-duplicate-filenames","text":"Causes the conversion stream to halt when multiple dataset items have the same filename","title":"CHECK-DUPLICATE-FILENAMES"},{"location":"plugins/#domains_24","text":"Image Segmentation Domain Image Object-Detection Domain Speech Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_24","text":"usage: check-duplicate-filenames","title":"Options:"},{"location":"plugins/#coerce-box","text":"Converts all annotation bounds into box regions","title":"COERCE-BOX"},{"location":"plugins/#domains_25","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_25","text":"usage: coerce-box","title":"Options:"},{"location":"plugins/#coerce-mask","text":"Converts all annotation bounds into polygon regions","title":"COERCE-MASK"},{"location":"plugins/#domains_26","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_26","text":"usage: coerce-mask","title":"Options:"},{"location":"plugins/#combine-annotations-od","text":"Combines object detection annotations from images passing through into a single annotation.","title":"COMBINE-ANNOTATIONS-OD"},{"location":"plugins/#domains_27","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_27","text":"usage: combine-annotations-od [--combination COMBINATION] [--min-iou MIN_IOU] optional arguments: --combination COMBINATION how to combine the annotations (union|intersect); the 'stream_index' key in the meta-data contains the stream index (default: intersect) --min-iou MIN_IOU the minimum IoU (intersect over union) to use for identifying objects that overlap (default: 0.7)","title":"Options:"},{"location":"plugins/#convert-image-format","text":"Converts images from one format to another","title":"CONVERT-IMAGE-FORMAT"},{"location":"plugins/#domains_28","text":"Image Segmentation Domain Image Object-Detection Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_28","text":"usage: convert-image-format -f FORMAT optional arguments: -f FORMAT, --format FORMAT format to convert images to (default: None)","title":"Options:"},{"location":"plugins/#crop","text":"Crops images.","title":"CROP"},{"location":"plugins/#domains_29","text":"Image Object-Detection Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_29","text":"usage: crop [-m IMGAUG_MODE] [--suffix IMGAUG_SUFFIX] [-f PERCENT_FROM] [-t PERCENT_TO] [-s SEED] [-a] [-T THRESHOLD] [-u] optional arguments: -m IMGAUG_MODE, --mode IMGAUG_MODE the image augmentation mode to use, available modes: replace, add (default: replace) --suffix IMGAUG_SUFFIX the suffix to use for the file names in case of augmentation mode add (default: None) -f PERCENT_FROM, --from-percent PERCENT_FROM the minimum percent to crop from images (default: None) -t PERCENT_TO, --to-percent PERCENT_TO the maximum percent to crop from images (default: None) -s SEED, --seed SEED the seed value to use for the random number generator; randomly seeded if not provided (default: None) -a, --seed-augmentation whether to seed the augmentation; if specified, uses the seeded random generator to produce a seed value from 0 to 1000 for the augmentation. (default: False) -T THRESHOLD, --threshold THRESHOLD the threshold to use for Random.rand(): if equal or above, augmentation gets applied; range: 0-1; default: 0 (= always) (default: None) -u, --update-size whether to update the image size after the crop operation or scale back to original size (default: False)","title":"Options:"},{"location":"plugins/#dimension-discarder","text":"Removes annotations which fall outside certain size constraints","title":"DIMENSION-DISCARDER"},{"location":"plugins/#domains_30","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_30","text":"usage: dimension-discarder [--max-area MAX_AREA] [--max-height MAX_HEIGHT] [--max-width MAX_WIDTH] [--min-area MIN_AREA] [--min-height MIN_HEIGHT] [--min-width MIN_WIDTH] [--verbose] optional arguments: --max-area MAX_AREA the maximum area of annotations to convert (default: None) --max-height MAX_HEIGHT the maximum height of annotations to convert (default: None) --max-width MAX_WIDTH the maximum width of annotations to convert (default: None) --min-area MIN_AREA the minimum area of annotations to convert (default: None) --min-height MIN_HEIGHT the minimum height of annotations to convert (default: None) --min-width MIN_WIDTH the minimum width of annotations to convert (default: None) --verbose outputs information when discarding annotations (default: False)","title":"Options:"},{"location":"plugins/#discard-invalid-images","text":"Discards images that cannot be loaded (e.g., corrupt image file or annotations with no image)","title":"DISCARD-INVALID-IMAGES"},{"location":"plugins/#domains_31","text":"Image Segmentation Domain Image Object-Detection Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_31","text":"usage: discard-invalid-images [-v] optional arguments: -v, --verbose whether to output debugging information (default: False)","title":"Options:"},{"location":"plugins/#discard-negatives","text":"Discards negative examples (those without annotations) from the stream","title":"DISCARD-NEGATIVES"},{"location":"plugins/#domains_32","text":"Image Segmentation Domain Image Object-Detection Domain Speech Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_32","text":"usage: discard-negatives","title":"Options:"},{"location":"plugins/#drop-frames","text":"Drops frames from the stream.","title":"DROP-FRAMES"},{"location":"plugins/#domains_33","text":"Image Segmentation Domain Image Object-Detection Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_33","text":"usage: drop-frames [-n NTH_FRAME] optional arguments: -n NTH_FRAME, --nth-frame NTH_FRAME which nth frame to drop, e..g, '2' means to drop every 2nd frame; passes frames through if <=1 (default: 0)","title":"Options:"},{"location":"plugins/#filter-frames-by-label-od","text":"Filters frames from the stream using the labels in the annotations, i.e., keeps or drops frames depending on presence/absence of labels.","title":"FILTER-FRAMES-BY-LABEL-OD"},{"location":"plugins/#domains_34","text":"Image Segmentation Domain Image Object-Detection Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_34","text":"usage: filter-frames-by-label-od [--excluded-labels EXCLUDED_LABELS] [--key-label KEY_LABEL] [--key-score KEY_SCORE] [--min-score MIN_SCORE] [--required-labels REQUIRED_LABELS] [-v] optional arguments: --excluded-labels EXCLUDED_LABELS the comma-separated list of labels that will automatically drop the frame when present in the frame (default: ) --key-label KEY_LABEL the meta-data key in the annotations that contains the label. (default: type) --key-score KEY_SCORE the meta-data key in the annotations to use for storing the prediction score. (default: score) --min-score MIN_SCORE the minimum score that predictions must have in order to be included in the label checks, ignored if not supplied (default: None) --required-labels REQUIRED_LABELS the comma-separated list of labels that must be present in the frame, otherwise it gets dropped (default: ) -v, --verbose whether to output debugging information. (default: False)","title":"Options:"},{"location":"plugins/#filter-labels","text":"Filters detected objects down to those with specified labels.","title":"FILTER-LABELS"},{"location":"plugins/#domains_35","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_35","text":"usage: filter-labels [-l LABELS [LABELS ...]] [-r regexp] optional arguments: -l LABELS [LABELS ...], --labels LABELS [LABELS ...] labels to use (default: []) -r regexp, --regexp regexp regular expression for using only a subset of labels (default: None)","title":"Options:"},{"location":"plugins/#filter-metadata","text":"Filters detected objects based on their meta-data.","title":"FILTER-METADATA"},{"location":"plugins/#domains_36","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_36","text":"usage: filter-metadata [-c COMPARISON] [-k KEY] [-t VALUE_TYPE] optional arguments: -c COMPARISON, --comparison COMPARISON the comparison to apply to the value: for bool/numeric/string '=OTHER' and '!=OTHER' can be used, for numeric furthermore '<OTHER', '<=OTHER', '>=OTHER', '>OTHER'. E.g.: '<3.0' for numeric types will discard any annotations that have a value of 3.0 or larger (default: None) -k KEY, --key KEY the key of the meta-data value to use for the filtering (default: None) -t VALUE_TYPE, --value-type VALUE_TYPE the data type that the value represents, available options: bool|numeric|string (default: None)","title":"Options:"},{"location":"plugins/#flip","text":"Flips images either left-to-right, up-to-down or both.","title":"FLIP"},{"location":"plugins/#domains_37","text":"Image Object-Detection Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_37","text":"usage: flip [-d DIRECTION] [-m IMGAUG_MODE] [--suffix IMGAUG_SUFFIX] [-s SEED] [-a] [-T THRESHOLD] optional arguments: -d DIRECTION, --direction DIRECTION the direction to flip, available options: lr, up, lrup (default: None) -m IMGAUG_MODE, --mode IMGAUG_MODE the image augmentation mode to use, available modes: replace, add (default: replace) --suffix IMGAUG_SUFFIX the suffix to use for the file names in case of augmentation mode add (default: None) -s SEED, --seed SEED the seed value to use for the random number generator; randomly seeded if not provided (default: None) -a, --seed-augmentation whether to seed the augmentation; if specified, uses the seeded random generator to produce a seed value from 0 to 1000 for the augmentation. (default: False) -T THRESHOLD, --threshold THRESHOLD the threshold to use for Random.rand(): if equal or above, augmentation gets applied; range: 0-1; default: 0 (= always) (default: None)","title":"Options:"},{"location":"plugins/#gaussian-blur","text":"Applies gaussian blur to images.","title":"GAUSSIAN-BLUR"},{"location":"plugins/#domains_38","text":"Image Object-Detection Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_38","text":"usage: gaussian-blur [-m IMGAUG_MODE] [--suffix IMGAUG_SUFFIX] [-s SEED] [-a] [-f SIGMA_FROM] [-t SIGMA_TO] [-T THRESHOLD] optional arguments: -m IMGAUG_MODE, --mode IMGAUG_MODE the image augmentation mode to use, available modes: replace, add (default: replace) --suffix IMGAUG_SUFFIX the suffix to use for the file names in case of augmentation mode add (default: None) -s SEED, --seed SEED the seed value to use for the random number generator; randomly seeded if not provided (default: None) -a, --seed-augmentation whether to seed the augmentation; if specified, uses the seeded random generator to produce a seed value from 0 to 1000 for the augmentation. (default: False) -f SIGMA_FROM, --from-sigma SIGMA_FROM the minimum sigma for the blur to apply to the images (default: None) -t SIGMA_TO, --to-sigma SIGMA_TO the maximum sigma for the blur to apply to the images (default: None) -T THRESHOLD, --threshold THRESHOLD the threshold to use for Random.rand(): if equal or above, augmentation gets applied; range: 0-1; default: 0 (= always) (default: None)","title":"Options:"},{"location":"plugins/#hsl-grayscale","text":"Turns RGB images into fake grayscale ones by converting them to HSL and then using the L channel for all channels. The brightness can be influenced and varied even.","title":"HSL-GRAYSCALE"},{"location":"plugins/#domains_39","text":"Image Object-Detection Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_39","text":"usage: hsl-grayscale [-f FACTOR_FROM] [-t FACTOR_TO] [-m IMGAUG_MODE] [--suffix IMGAUG_SUFFIX] [-s SEED] [-a] [-T THRESHOLD] optional arguments: -f FACTOR_FROM, --from-factor FACTOR_FROM the start of the factor range to apply to the L channel to darken or lighten the image (<1: darker, >1: lighter) (default: None) -t FACTOR_TO, --to-factor FACTOR_TO the end of the factor range to apply to the L channel to darken or lighten the image (<1: darker, >1: lighter) (default: None) -m IMGAUG_MODE, --mode IMGAUG_MODE the image augmentation mode to use, available modes: replace, add (default: replace) --suffix IMGAUG_SUFFIX the suffix to use for the file names in case of augmentation mode add (default: None) -s SEED, --seed SEED the seed value to use for the random number generator; randomly seeded if not provided (default: None) -a, --seed-augmentation whether to seed the augmentation; if specified, uses the seeded random generator to produce a seed value from 0 to 1000 for the augmentation. (default: False) -T THRESHOLD, --threshold THRESHOLD the threshold to use for Random.rand(): if equal or above, augmentation gets applied; range: 0-1; default: 0 (= always) (default: None)","title":"Options:"},{"location":"plugins/#linear-contrast","text":"Applies linear contrast to images.","title":"LINEAR-CONTRAST"},{"location":"plugins/#domains_40","text":"Image Object-Detection Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_40","text":"usage: linear-contrast [-f ALPHA_FROM] [-t ALPHA_TO] [-m IMGAUG_MODE] [--suffix IMGAUG_SUFFIX] [-s SEED] [-a] [-T THRESHOLD] optional arguments: -f ALPHA_FROM, --from-alpha ALPHA_FROM the minimum alpha to apply to the images (default: None) -t ALPHA_TO, --to-alpha ALPHA_TO the maximum alpha to apply to the images (default: None) -m IMGAUG_MODE, --mode IMGAUG_MODE the image augmentation mode to use, available modes: replace, add (default: replace) --suffix IMGAUG_SUFFIX the suffix to use for the file names in case of augmentation mode add (default: None) -s SEED, --seed SEED the seed value to use for the random number generator; randomly seeded if not provided (default: None) -a, --seed-augmentation whether to seed the augmentation; if specified, uses the seeded random generator to produce a seed value from 0 to 1000 for the augmentation. (default: False) -T THRESHOLD, --threshold THRESHOLD the threshold to use for Random.rand(): if equal or above, augmentation gets applied; range: 0-1; default: 0 (= always) (default: None)","title":"Options:"},{"location":"plugins/#map-labels","text":"Maps object-detection labels from one set to another","title":"MAP-LABELS"},{"location":"plugins/#domains_41","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_41","text":"usage: map-labels [-m old=new] optional arguments: -m old=new, --mapping old=new mapping for labels, for replacing one label string with another (eg when fixing/collapsing labels) (default: [])","title":"Options:"},{"location":"plugins/#od-to-ic","text":"Converts image object-detection instances into image classification instances","title":"OD-TO-IC"},{"location":"plugins/#domains_42","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_42","text":"usage: od-to-ic [-m HANDLER] optional arguments: -m HANDLER, --multiplicity HANDLER how to handle instances with more than one located object (default: error)","title":"Options:"},{"location":"plugins/#od-to-is","text":"Converts image object-detection instances into image segmentation instances","title":"OD-TO-IS"},{"location":"plugins/#domains_43","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_43","text":"usage: od-to-is [--label-error] --labels LABEL [LABEL ...] optional arguments: --label-error whether to raise errors when an unspecified label is encountered (default is to ignore) (default: False) --labels LABEL [LABEL ...] specifies the labels for each index (default: None)","title":"Options:"},{"location":"plugins/#passthrough","text":"Dummy ISP which has no effect on the conversion stream","title":"PASSTHROUGH"},{"location":"plugins/#domains_44","text":"Image Segmentation Domain Image Object-Detection Domain Speech Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_44","text":"usage: passthrough","title":"Options:"},{"location":"plugins/#polygon-discarder","text":"Removes annotations with polygons which fall outside certain point limit constraints","title":"POLYGON-DISCARDER"},{"location":"plugins/#domains_45","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_45","text":"usage: polygon-discarder [--max-points MAX_POINTS] [--min-points MIN_POINTS] [--verbose] optional arguments: --max-points MAX_POINTS the maximum number of points in the polygon (default: None) --min-points MIN_POINTS the minimum number of points in the polygon (default: None) --verbose outputs information when discarding annotations (default: False)","title":"Options:"},{"location":"plugins/#redis-predict-ic","text":"Makes image classification predictions via Redis backend, passing in an image and receiving JSON predictions back (at least one of 'label: probability'). Predictions example: {\"dog\": 0.9, \"cat\": 0.1}","title":"REDIS-PREDICT-IC"},{"location":"plugins/#domains_46","text":"Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_46","text":"usage: redis-predict-ic [--channel-in CHANNEL_IN] [--channel-out CHANNEL_OUT] [-d REDIS_DB] [-h REDIS_HOST] [-p REDIS_PORT] [-t TIMEOUT] [-v] optional arguments: --channel-in CHANNEL_IN the Redis channel on which to receive predictions. (default: predictions) --channel-out CHANNEL_OUT the Redis channel to send the images out (default: images) -d REDIS_DB, --redis-db REDIS_DB the database to use (default: 0) -h REDIS_HOST, --redis-host REDIS_HOST the Redis server to connect to (default: localhost) -p REDIS_PORT, --redis-port REDIS_PORT the port the Redis server is running on (default: 6379) -t TIMEOUT, --timeout TIMEOUT the timeout in seconds to wait for a prediction to arrive (default: 5.0) -v, --verbose whether to output debugging information. (default: False)","title":"Options:"},{"location":"plugins/#redis-predict-is","text":"Makes image segmentation predictions via Redis backend, passing in an image and receiving an image with predicted segmentations.","title":"REDIS-PREDICT-IS"},{"location":"plugins/#domains_47","text":"Image Segmentation Domain","title":"Domain(s):"},{"location":"plugins/#options_47","text":"usage: redis-predict-is [--channel-in CHANNEL_IN] [--channel-out CHANNEL_OUT] [--image-format IMAGE_FORMAT] --labels LABEL [LABEL ...] [-d REDIS_DB] [-h REDIS_HOST] [-p REDIS_PORT] [-t TIMEOUT] [-v] optional arguments: --channel-in CHANNEL_IN the Redis channel on which to receive predictions. (default: predictions) --channel-out CHANNEL_OUT the Redis channel to send the images out (default: images) --image-format IMAGE_FORMAT the format of the image that comes back as prediction: indexedpng,bluechannel,grayscale (default: indexedpng) --labels LABEL [LABEL ...] specifies the labels for each index (default: None) -d REDIS_DB, --redis-db REDIS_DB the database to use (default: 0) -h REDIS_HOST, --redis-host REDIS_HOST the Redis server to connect to (default: localhost) -p REDIS_PORT, --redis-port REDIS_PORT the port the Redis server is running on (default: 6379) -t TIMEOUT, --timeout TIMEOUT the timeout in seconds to wait for a prediction to arrive (default: 5.0) -v, --verbose whether to output debugging information. (default: False)","title":"Options:"},{"location":"plugins/#redis-predict-od","text":"Makes object detection predictions via Redis backend, passing in an image and receiving OPEX predictions back: https://github.com/WaikatoLink2020/objdet-predictions-exchange-format","title":"REDIS-PREDICT-OD"},{"location":"plugins/#domains_48","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_48","text":"usage: redis-predict-od [--channel-in CHANNEL_IN] [--channel-out CHANNEL_OUT] [--key-label KEY_LABEL] [--key-score KEY_SCORE] [-d REDIS_DB] [-h REDIS_HOST] [-p REDIS_PORT] [-t TIMEOUT] [-v] optional arguments: --channel-in CHANNEL_IN the Redis channel on which to receive predictions. (default: predictions) --channel-out CHANNEL_OUT the Redis channel to send the images out (default: images) --key-label KEY_LABEL the meta-data key in the annotations to use for storing the label. (default: type) --key-score KEY_SCORE the meta-data key in the annotations to use for storing the prediction score. (default: score) -d REDIS_DB, --redis-db REDIS_DB the database to use (default: 0) -h REDIS_HOST, --redis-host REDIS_HOST the Redis server to connect to (default: localhost) -p REDIS_PORT, --redis-port REDIS_PORT the port the Redis server is running on (default: 6379) -t TIMEOUT, --timeout TIMEOUT the timeout in seconds to wait for a prediction to arrive (default: 5.0) -v, --verbose whether to output debugging information. (default: False)","title":"Options:"},{"location":"plugins/#remove-classes","text":"Removes classes from classification/image-segmentation instances","title":"REMOVE-CLASSES"},{"location":"plugins/#domains_49","text":"Image Segmentation Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_49","text":"usage: remove-classes -c CLASS [CLASS ...] optional arguments: -c CLASS [CLASS ...], --classes CLASS [CLASS ...] the classes to remove (default: None)","title":"Options:"},{"location":"plugins/#rotate","text":"Rotates images randomly within a range of degrees or by a specified degree. Specify seed value and force augmentation to be seeded to generate repeatable augmentations.","title":"ROTATE"},{"location":"plugins/#domains_50","text":"Image Object-Detection Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_50","text":"usage: rotate [-f DEGREE_FROM] [-t DEGREE_TO] [-m IMGAUG_MODE] [--suffix IMGAUG_SUFFIX] [-s SEED] [-a] [-T THRESHOLD] optional arguments: -f DEGREE_FROM, --from-degree DEGREE_FROM the start of the degree range to use for rotating the images (default: None) -t DEGREE_TO, --to-degree DEGREE_TO the end of the degree range to use for rotating the images (default: None) -m IMGAUG_MODE, --mode IMGAUG_MODE the image augmentation mode to use, available modes: replace, add (default: replace) --suffix IMGAUG_SUFFIX the suffix to use for the file names in case of augmentation mode add (default: None) -s SEED, --seed SEED the seed value to use for the random number generator; randomly seeded if not provided (default: None) -a, --seed-augmentation whether to seed the augmentation; if specified, uses the seeded random generator to produce a seed value from 0 to 1000 for the augmentation. (default: False) -T THRESHOLD, --threshold THRESHOLD the threshold to use for Random.rand(): if equal or above, augmentation gets applied; range: 0-1; default: 0 (= always) (default: None)","title":"Options:"},{"location":"plugins/#scale","text":"Scales images randomly within a range of percentages or by a specified percentage. Specify seed value and force augmentation to be seeded to generate repeatable augmentations.","title":"SCALE"},{"location":"plugins/#domains_51","text":"Image Object-Detection Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_51","text":"usage: scale [-m IMGAUG_MODE] [--suffix IMGAUG_SUFFIX] [-k] [-f PERCENTAGE_FROM] [-t PERCENTAGE_TO] [-s SEED] [-a] [-T THRESHOLD] [-u] optional arguments: -m IMGAUG_MODE, --mode IMGAUG_MODE the image augmentation mode to use, available modes: replace, add (default: replace) --suffix IMGAUG_SUFFIX the suffix to use for the file names in case of augmentation mode add (default: None) -k, --keep-aspect whether to keep the aspect ratio (default: False) -f PERCENTAGE_FROM, --from-percentage PERCENTAGE_FROM the start of the percentage range to use for scaling the images (default: None) -t PERCENTAGE_TO, --to-percentage PERCENTAGE_TO the end of the percentage range to use for scaling the images (default: None) -s SEED, --seed SEED the seed value to use for the random number generator; randomly seeded if not provided (default: None) -a, --seed-augmentation whether to seed the augmentation; if specified, uses the seeded random generator to produce a seed value from 0 to 1000 for the augmentation. (default: False) -T THRESHOLD, --threshold THRESHOLD the threshold to use for Random.rand(): if equal or above, augmentation gets applied; range: 0-1; default: 0 (= always) (default: None) -u, --update-size whether to update the image size after the scaling operation or use original size (default: False)","title":"Options:"},{"location":"plugins/#skip-similar-frames","text":"Skips frames in the stream that are deemed too similar.","title":"SKIP-SIMILAR-FRAMES"},{"location":"plugins/#domains_52","text":"Image Segmentation Domain Image Object-Detection Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_52","text":"usage: skip-similar-frames [-b BW_THRESHOLD] [-t CHANGE_THRESHOLD] [-c CONVERSION] [-v] optional arguments: -b BW_THRESHOLD, --bw-threshold BW_THRESHOLD the threshold to use for converting a gray-scale like image to black and white (0-255) (default: 128) -t CHANGE_THRESHOLD, --change-threshold CHANGE_THRESHOLD the percentage of pixels that changed relative to size of image (0-1) (default: 0.01) -c CONVERSION, --conversion CONVERSION how to convert the BGR image to a single channel image (gray/r/g/b) (default: gray) -v, --verbose whether to output some debugging output. (default: False)","title":"Options:"},{"location":"plugins/#strip-annotations","text":"ISP which removes annotations from instances","title":"STRIP-ANNOTATIONS"},{"location":"plugins/#domains_53","text":"Image Segmentation Domain Image Object-Detection Domain Speech Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_53","text":"usage: strip-annotations","title":"Options:"},{"location":"plugins/#sub-images","text":"Extracts sub-images (incl their annotations) from the images coming through, using the defined regions.","title":"SUB-IMAGES"},{"location":"plugins/#domains_54","text":"Image Object-Detection Domain Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_54","text":"usage: sub-images [-p] [-s REGION_SORTING] [-r REGIONS [REGIONS ...]] [-e] optional arguments: -p, --include-partial whether to include only annotations that fit fully into a region or also partial ones (default: False) -s REGION_SORTING, --region-sorting REGION_SORTING how to sort the supplied region definitions: none|x-then-y|y-then-x (default: none) -r REGIONS [REGIONS ...], --regions REGIONS [REGIONS ...] the regions (X,Y,WIDTH,HEIGHT) to crop and forward with their annotations (default: []) -e, --suppress-empty suppresses sub-images that have no annotations (object detection) (default: False)","title":"Options:"},{"location":"plugins/#sink-stage","text":"","title":"Sink stage"},{"location":"plugins/#area-histogram-is","text":"Generates histograms of the area (normalized or absolute) occupied by the annotations.","title":"AREA-HISTOGRAM-IS"},{"location":"plugins/#domains_55","text":"Image Segmentation Domain","title":"Domain(s):"},{"location":"plugins/#options_55","text":"usage: area-histogram-is [-a ALL_LABEL] [-b] [--label-key LABEL_KEY] [-n] [--num-bins NUM_BINS] [-o OUTPUT_FILE] [-f OUTPUT_FORMAT] optional arguments: -a ALL_LABEL, --all-label ALL_LABEL the label to use for all the labels combined (default: ALL) -b, --force-bbox whether to use the bounding box even if a polygon is present (object detection domain only) (default: False) --label-key LABEL_KEY the key in the meta-data that contains the label. (default: type) -n, --normalized whether to use normalized areas (using the image size as base). (default: False) --num-bins NUM_BINS the number of bins to use for the histogram. (default: 20) -o OUTPUT_FILE, --output OUTPUT_FILE the file to write the histogram to; uses stdout if omitted (default: ) -f OUTPUT_FORMAT, --format OUTPUT_FORMAT the format to use for the output, available modes: csv, json (default: text)","title":"Options:"},{"location":"plugins/#area-histogram-od","text":"Generates histograms of the area (normalized or absolute) occupied by the annotations.","title":"AREA-HISTOGRAM-OD"},{"location":"plugins/#domains_56","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_56","text":"usage: area-histogram-od [-a ALL_LABEL] [-b] [--label-key LABEL_KEY] [-n] [--num-bins NUM_BINS] [-o OUTPUT_FILE] [-f OUTPUT_FORMAT] optional arguments: -a ALL_LABEL, --all-label ALL_LABEL the label to use for all the labels combined (default: ALL) -b, --force-bbox whether to use the bounding box even if a polygon is present (object detection domain only) (default: False) --label-key LABEL_KEY the key in the meta-data that contains the label. (default: type) -n, --normalized whether to use normalized areas (using the image size as base). (default: False) --num-bins NUM_BINS the number of bins to use for the histogram. (default: 20) -o OUTPUT_FILE, --output OUTPUT_FILE the file to write the histogram to; uses stdout if omitted (default: ) -f OUTPUT_FORMAT, --format OUTPUT_FORMAT the format to use for the output, available modes: csv, json (default: text)","title":"Options:"},{"location":"plugins/#calc-frame-changes","text":"Calculates the changes between frames, which can be used with the skip-similar-frames ISP.","title":"CALC-FRAME-CHANGES"},{"location":"plugins/#domains_57","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_57","text":"usage: calc-frame-changes [-b BW_THRESHOLD] [-t CHANGE_THRESHOLD] [-c CONVERSION] [-B NUM_BINS] [-o OUTPUT_FILE] [-f OUTPUT_FORMAT] [-v] optional arguments: -b BW_THRESHOLD, --bw-threshold BW_THRESHOLD the threshold to use for converting a gray-scale like image to black and white (0-255) (default: 128) -t CHANGE_THRESHOLD, --change-threshold CHANGE_THRESHOLD the percentage of pixels that changed relative to size of image (0-1) (default: 0.01) -c CONVERSION, --conversion CONVERSION how to convert the BGR image to a single channel image (gray/r/g/b) (default: gray) -B NUM_BINS, --num-bins NUM_BINS the number of bins to use for the histogram (default: 20) -o OUTPUT_FILE, --output OUTPUT_FILE the file to write to statistics to, stdout if not provided (default: ) -f OUTPUT_FORMAT, --output-format OUTPUT_FORMAT how to output the statistics (text/csv/json) (default: text) -v, --verbose whether to output some debugging output. (default: False)","title":"Options:"},{"location":"plugins/#image-viewer-ic","text":"Displays image classification images.","title":"IMAGE-VIEWER-IC"},{"location":"plugins/#domains_58","text":"Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_58","text":"usage: image-viewer-ic [--delay DELAY] [--position POSITION] [--size SIZE] [--title TITLE] optional arguments: --delay DELAY the delay in milli-seconds between images, use 0 to wait for keypress, ignored if <0 (default: 500) --position POSITION the position of the window on screen (X,Y) (default: 0,0) --size SIZE the maximum size for the image: WIDTH,HEIGHT (default: 640,480) --title TITLE the title for the window (default: wai.annotations)","title":"Options:"},{"location":"plugins/#image-viewer-is","text":"Displays image segmentation images.","title":"IMAGE-VIEWER-IS"},{"location":"plugins/#domains_59","text":"Image Segmentation Domain","title":"Domain(s):"},{"location":"plugins/#options_59","text":"usage: image-viewer-is [--delay DELAY] [--position POSITION] [--size SIZE] [--title TITLE] optional arguments: --delay DELAY the delay in milli-seconds between images, use 0 to wait for keypress, ignored if <0 (default: 500) --position POSITION the position of the window on screen (X,Y) (default: 0,0) --size SIZE the maximum size for the image: WIDTH,HEIGHT (default: 640,480) --title TITLE the title for the window (default: wai.annotations)","title":"Options:"},{"location":"plugins/#image-viewer-od","text":"Displays object detection images.","title":"IMAGE-VIEWER-OD"},{"location":"plugins/#domains_60","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_60","text":"usage: image-viewer-od [--delay DELAY] [--position POSITION] [--size SIZE] [--title TITLE] optional arguments: --delay DELAY the delay in milli-seconds between images, use 0 to wait for keypress, ignored if <0 (default: 500) --position POSITION the position of the window on screen (X,Y) (default: 0,0) --size SIZE the maximum size for the image: WIDTH,HEIGHT (default: 640,480) --title TITLE the title for the window (default: wai.annotations)","title":"Options:"},{"location":"plugins/#label-dist-ic","text":"Generates a label distribution.","title":"LABEL-DIST-IC"},{"location":"plugins/#domains_61","text":"Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_61","text":"usage: label-dist-ic [--label-key LABEL_KEY] [-o OUTPUT_FILE] [-f OUTPUT_FORMAT] [-p] optional arguments: --label-key LABEL_KEY the key in the meta-data that contains the label. (default: type) -o OUTPUT_FILE, --output OUTPUT_FILE the file to write the statistics to; uses stdout if omitted (default: ) -f OUTPUT_FORMAT, --format OUTPUT_FORMAT the format to use for the output, available modes: csv, json (default: text) -p, --percentages whether to output percentages instead of counts. (default: False)","title":"Options:"},{"location":"plugins/#label-dist-is","text":"Generates a label distribution.","title":"LABEL-DIST-IS"},{"location":"plugins/#domains_62","text":"Image Segmentation Domain","title":"Domain(s):"},{"location":"plugins/#options_62","text":"usage: label-dist-is [--label-key LABEL_KEY] [-o OUTPUT_FILE] [-f OUTPUT_FORMAT] [-p] optional arguments: --label-key LABEL_KEY the key in the meta-data that contains the label. (default: type) -o OUTPUT_FILE, --output OUTPUT_FILE the file to write the statistics to; uses stdout if omitted (default: ) -f OUTPUT_FORMAT, --format OUTPUT_FORMAT the format to use for the output, available modes: csv, json (default: text) -p, --percentages whether to output percentages instead of counts. (default: False)","title":"Options:"},{"location":"plugins/#label-dist-od","text":"Generates a label distribution.","title":"LABEL-DIST-OD"},{"location":"plugins/#domains_63","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_63","text":"usage: label-dist-od [--label-key LABEL_KEY] [-o OUTPUT_FILE] [-f OUTPUT_FORMAT] [-p] optional arguments: --label-key LABEL_KEY the key in the meta-data that contains the label. (default: type) -o OUTPUT_FILE, --output OUTPUT_FILE the file to write the statistics to; uses stdout if omitted (default: ) -f OUTPUT_FORMAT, --format OUTPUT_FORMAT the format to use for the output, available modes: csv, json (default: text) -p, --percentages whether to output percentages instead of counts. (default: False)","title":"Options:"},{"location":"plugins/#to-adams-ic","text":"Writes image classification annotations in the ADAMS report-format","title":"TO-ADAMS-IC"},{"location":"plugins/#domains_64","text":"Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_64","text":"usage: to-adams-ic -c FIELD [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: -c FIELD, --class-field FIELD the report field containing the image class (default: None) --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH output directory to write files to (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: [])","title":"Options:"},{"location":"plugins/#to-adams-od","text":"Writes image object-detection annotations in the ADAMS report-format","title":"TO-ADAMS-OD"},{"location":"plugins/#domains_65","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_65","text":"usage: to-adams-od [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH output directory to write files to (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: [])","title":"Options:"},{"location":"plugins/#to-annotation-overlay-od","text":"Generates an image with all the annotation shapes (bbox or polygon) overlayed.","title":"TO-ANNOTATION-OVERLAY-OD"},{"location":"plugins/#domains_66","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_66","text":"usage: to-annotation-overlay-od [-b BACKGROUND_COLOR] [-c COLOR] [-o OUTPUT_FILE] [-s SCALE_TO] optional arguments: -b BACKGROUND_COLOR, --background-color BACKGROUND_COLOR the color to use for the background as RGBA byte-quadruplet, e.g.: 255,255,255,255 (default: 255,255,255,255) -c COLOR, --color COLOR the color to use for drawing the shapes as RGBA byte-quadruplet, e.g.: 255,0,0,64 (default: 255,0,0,64) -o OUTPUT_FILE, --output OUTPUT_FILE the PNG image to write the generated overlay to (default: ./overlay.png) -s SCALE_TO, --scale-to SCALE_TO the dimensions to scale all images to before overlaying them (format: width,height) (default: )","title":"Options:"},{"location":"plugins/#to-blue-channel-is","text":"Writes image segmentation files in the blue-channel format","title":"TO-BLUE-CHANNEL-IS"},{"location":"plugins/#domains_67","text":"Image Segmentation Domain","title":"Domain(s):"},{"location":"plugins/#options_67","text":"usage: to-blue-channel-is [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH the directory to write the annotation images to (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: [])","title":"Options:"},{"location":"plugins/#to-coco-od","text":"Writes image object-detection annotations in the MS-COCO JSON-format","title":"TO-COCO-OD"},{"location":"plugins/#domains_68","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_68","text":"usage: to-coco-od [--annotations-only] [--categories CATEGORY [CATEGORY ...]] [--category-output-file FILENAME] [--default-supercategory SUPERCATEGORY] [--error-on-new-category] [--license-name LICENSE_NAME] [--license-url LICENSE_URL] -o PATH [--pretty] [--sort-categories] [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) --categories CATEGORY [CATEGORY ...] defines the order of the categories (default: []) --category-output-file FILENAME file to write the categories into, as a simple comma-separated list (default: None) --default-supercategory SUPERCATEGORY the supercategory to use for pre-defined categories (default: Object) --error-on-new-category whether unspecified categories should raise an error (default: False) --license-name LICENSE_NAME the license of the images (default: default) --license-url LICENSE_URL the license of the images (default: ) -o PATH, --output PATH output file to write annotations to (images are placed in same directory) (default: None) --pretty whether to format the JSON annotations file with indentation (default: False) --sort-categories whether to put the categories in alphabetical order (default: False) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: [])","title":"Options:"},{"location":"plugins/#to-common-voice-sp","text":"Writes speech transcriptions in the Mozilla Common-Voice TSV-format","title":"TO-COMMON-VOICE-SP"},{"location":"plugins/#domains_69","text":"Speech Domain","title":"Domain(s):"},{"location":"plugins/#options_69","text":"usage: to-common-voice-sp [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH the filename of the TSV file to write the annotations into (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: [])","title":"Options:"},{"location":"plugins/#to-festvox-sp","text":"Writes speech transcriptions in the Festival FestVox format","title":"TO-FESTVOX-SP"},{"location":"plugins/#domains_70","text":"Speech Domain","title":"Domain(s):"},{"location":"plugins/#options_70","text":"usage: to-festvox-sp [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH the filename of the FestVox file to write the annotations into (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: [])","title":"Options:"},{"location":"plugins/#to-grayscale-is","text":"Writes image segmentation files in the grayscale format","title":"TO-GRAYSCALE-IS"},{"location":"plugins/#domains_71","text":"Image Segmentation Domain","title":"Domain(s):"},{"location":"plugins/#options_71","text":"usage: to-grayscale-is [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH the directory to write the annotation images to (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: [])","title":"Options:"},{"location":"plugins/#to-images-ic","text":"Dummy writer that just outputs images from image classification datasets.","title":"TO-IMAGES-IC"},{"location":"plugins/#domains_72","text":"Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_72","text":"usage: to-images-ic [-o OUTPUT_DIR] optional arguments: -o OUTPUT_DIR, --output-dir OUTPUT_DIR the directory to write the images to (default: .)","title":"Options:"},{"location":"plugins/#to-images-is","text":"Dummy writer that just outputs images from image segmentation datasets.","title":"TO-IMAGES-IS"},{"location":"plugins/#domains_73","text":"Image Segmentation Domain","title":"Domain(s):"},{"location":"plugins/#options_73","text":"usage: to-images-is [-o OUTPUT_DIR] optional arguments: -o OUTPUT_DIR, --output-dir OUTPUT_DIR the directory to write the images to (default: .)","title":"Options:"},{"location":"plugins/#to-images-od","text":"Dummy writer that just outputs images from object detection datasets.","title":"TO-IMAGES-OD"},{"location":"plugins/#domains_74","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_74","text":"usage: to-images-od [-o OUTPUT_DIR] optional arguments: -o OUTPUT_DIR, --output-dir OUTPUT_DIR the directory to write the images to (default: .)","title":"Options:"},{"location":"plugins/#to-indexed-png-is","text":"Writes image segmentation files in the indexed-PNG format","title":"TO-INDEXED-PNG-IS"},{"location":"plugins/#domains_75","text":"Image Segmentation Domain","title":"Domain(s):"},{"location":"plugins/#options_75","text":"usage: to-indexed-png-is [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH the directory to write the annotation images to (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: [])","title":"Options:"},{"location":"plugins/#to-layer-segments-is","text":"Writes the layer-segments image-segmentation format to disk","title":"TO-LAYER-SEGMENTS-IS"},{"location":"plugins/#domains_76","text":"Image Segmentation Domain","title":"Domain(s):"},{"location":"plugins/#options_76","text":"usage: to-layer-segments-is [--annotations-only] [--label-separator SEPARATOR] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) --label-separator SEPARATOR the separator between the base filename and the label (default: -) -o PATH, --output PATH the directory to write the annotation images to (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: [])","title":"Options:"},{"location":"plugins/#to-opex-od","text":"Writes image object-detection annotations in the OPEX format","title":"TO-OPEX-OD"},{"location":"plugins/#domains_77","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_77","text":"usage: to-opex-od [-c PATH] [-l PATH] [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: -c PATH, --labels-csv PATH Path to the labels CSV file to write (default: None) -l PATH, --labels PATH Path to the labels file to write (default: None) --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH output directory to write images and annotations to (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: [])","title":"Options:"},{"location":"plugins/#to-roi-od","text":"Writes image object-detection annotations in the ROI CSV-format","title":"TO-ROI-OD"},{"location":"plugins/#domains_78","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_78","text":"usage: to-roi-od [-d WIDTH HEIGHT] [--annotations-only] [--comments COMMENTS [COMMENTS ...]] -o PATH [--size-mode] [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] [--prefix WRITER_PREFIX] [--suffix WRITER_SUFFIX] optional arguments: -d WIDTH HEIGHT, --image-dimensions WIDTH HEIGHT image dimensions to use if none can be inferred (default: []) --annotations-only skip the writing of data files, outputting only the annotation files (default: False) --comments COMMENTS [COMMENTS ...] comments to write to the beginning of the ROI file (default: []) -o PATH, --output PATH output directory to write files to (default: None) --size-mode writes the ROI files with x,y,w,h headers instead of x0,y0,x1,y1 (default: False) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: []) --prefix WRITER_PREFIX the prefix for output filenames (default = '') (default: None) --suffix WRITER_SUFFIX the suffix for output filenames (default = '-rois.csv') (default: None)","title":"Options:"},{"location":"plugins/#to-subdir-ic","text":"Writes images to sub-directories named after their class labels.","title":"TO-SUBDIR-IC"},{"location":"plugins/#domains_79","text":"Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_79","text":"usage: to-subdir-ic -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: -o PATH, --output PATH the directory to store the class directories in (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: [])","title":"Options:"},{"location":"plugins/#to-tf-od","text":"Writes image object-detection annotations in the TFRecords binary format","title":"TO-TF-OD"},{"location":"plugins/#domains_80","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_80","text":"usage: to-tf-od [--dense] [--source-id-type {filename,numeric-dummy}] -o PATH [-p FILENAME] [-s FILENAME [FILENAME ...]] [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --dense outputs masks in the dense numerical format instead of PNG-encoded (default: False) --source-id-type {filename,numeric-dummy} by default, the filename gets stored in the 'source_id' field, but some algorithms try to convert it into a number and fail with 'StringToNumberOp could not correctly convert string'; in which case you can use 'numeric- dummy' (see https://github.com/google/automl/issues/307) (default: filename) -o PATH, --output PATH name of output file for TFRecords (default: None) -p FILENAME, --protobuf FILENAME for storing the label strings and IDs (default: None) -s FILENAME [FILENAME ...], --shards FILENAME [FILENAME ...] additional shards to write to (default: []) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: [])","title":"Options:"},{"location":"plugins/#to-vgg-od","text":"Writes image object-detection annotations in the VGG JSON-format","title":"TO-VGG-OD"},{"location":"plugins/#domains_81","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_81","text":"usage: to-vgg-od [--annotations-only] -o PATH [--pretty] [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH output file to write annotations to (images are placed in same directory) (default: None) --pretty whether to format the JSON annotations file with indentation (default: False) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: [])","title":"Options:"},{"location":"plugins/#to-video-file-od","text":"Writes frames to a MJPG video file.","title":"TO-VIDEO-FILE-OD"},{"location":"plugins/#domains_82","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_82","text":"usage: to-video-file-od [-f FPS] [-o OUTPUT_FILE] optional arguments: -f FPS, --fps FPS the frames per second to use (default: 25) -o OUTPUT_FILE, --output OUTPUT_FILE the MJPG video file to write to (default: )","title":"Options:"},{"location":"plugins/#to-voc-od","text":"Writes image object-detection annotations in the Pascal VOC XML-format","title":"TO-VOC-OD"},{"location":"plugins/#domains_83","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_83","text":"usage: to-voc-od [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH output directory to write annotations to (images are placed in same directory) (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: [])","title":"Options:"},{"location":"plugins/#to-void-ic","text":"Consumes instances without writing them.","title":"TO-VOID-IC"},{"location":"plugins/#domains_84","text":"Image Classification Domain","title":"Domain(s):"},{"location":"plugins/#options_84","text":"usage: to-void-ic","title":"Options:"},{"location":"plugins/#to-void-is","text":"Consumes instances without writing them.","title":"TO-VOID-IS"},{"location":"plugins/#domains_85","text":"Image Segmentation Domain","title":"Domain(s):"},{"location":"plugins/#options_85","text":"usage: to-void-is","title":"Options:"},{"location":"plugins/#to-void-od","text":"Consumes instances without writing them.","title":"TO-VOID-OD"},{"location":"plugins/#domains_86","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_86","text":"usage: to-void-od","title":"Options:"},{"location":"plugins/#to-void-sp","text":"Consumes instances without writing them.","title":"TO-VOID-SP"},{"location":"plugins/#domains_87","text":"Speech Domain","title":"Domain(s):"},{"location":"plugins/#options_87","text":"usage: to-void-sp","title":"Options:"},{"location":"plugins/#to-yolo-od","text":"Writes image object-detection annotations in the YOLO format","title":"TO-YOLO-OD"},{"location":"plugins/#domains_88","text":"Image Object-Detection Domain","title":"Domain(s):"},{"location":"plugins/#options_88","text":"usage: to-yolo-od [-c PATH] [-l PATH] [--annotations-only] -o PATH [--split-names SPLIT NAME [SPLIT NAME ...]] [--split-ratios RATIO [RATIO ...]] optional arguments: -c PATH, --labels-csv PATH Path to the labels CSV file to write (default: None) -l PATH, --labels PATH Path to the labels file to write (default: None) --annotations-only skip the writing of data files, outputting only the annotation files (default: False) -o PATH, --output PATH output directory to write images and annotations to (default: None) --split-names SPLIT NAME [SPLIT NAME ...] the names to use for the splits (default: []) --split-ratios RATIO [RATIO ...] the ratios to use for the splits (default: [])","title":"Options:"},{"location":"stream/","text":"Stream Layer The stream layer provides classes for creating custom stream-processing pipelines. Each pipeline consists of: a source, which injects items into the pipeline for processing; processors, which take items and process them; and a sink, which consumes the items coming out of the pipeline. There are 3 base-classes representing sources, processors and sinks (respectively: StreamSource, StreamProcessor and StreamSink). These classes should be inherited to provide customised stream-processing functionality. The Pipeline class gathers the individual stream elements into a whole, connects each element to the previous and next elements, and ensures that the elements follow the pipeline's calling semantics (see below). Note that while the stream-processing elements are typed on the items they consume/produce, this is only for linting purposes and not enforced at run-time. Calling semantics For sources and processors, the ability to pass items on to later elements in the processing pipeline is provided through 2 callback functions, then and done . then forwards an item to the next element of the pipeline, and done signals that no more items will be forwarded. This design was chosen over e.g. iterators as it ensures that when an error occurs, the relevant information which led to the error is retained on the call-stack. This is a useful aid for debugging. However, this does mean that the potential exists for the callbacks to called in an arbitrary order, when only certain permutations are sensible. The intended calling semantics of the 2 callbacks are: 0 or more calls to then to forward items, followed by exactly 1 call to done to signal termination. The pipeline class constructs these callbacks so that, if they are not called in the correct order, an error occurs. Specifically, calling then after done has been called will raise an error, as will not calling done at all. In practice, done is written to be idempotent, such that calling it multiple times is the same as calling it once. StreamSource Stream sources should implement the produce method to inject items into the pipeline. The method should call the then callback with each item to inject as its argument. Once all items have been produced, the source should then call the done callback. StreamProcessor Stream processors should implement process_element method to receive items from the previous processing-element in the pipeline for processing. They should also implement the finish method to finalise processing after all items have been received. Both methods take the then and done callbacks as arguments, so processors can process items and forward the processed results as they arrive, or batch them and perform processing/forwarding in bulk. An optional start method is available, which can be overridden to perform any set-up required before the processor starts receiving items. StreamSink Stream sinks should implement the consume_element method to consume items produced by the pipeline. They should also implement the finish method to perform any tidy-up once the pipeline has terminated. An optional start method is available, which can be overridden to perform any set-up required before the sink starts receiving items. Pipeline The Pipeline class take an optional source, zero or more processors, and an optional sink as its constructor arguments. When the process method is called, the processing elements are connected together, the start methods for each processor and the sink are called, and the source is instructed to start producing items. Optionally, an iterable can be passed to the process method to replace the pipeline's own source, and/or a function can be passed to replace the pipeline's sink. When the pipeline is connecting the processing elements together, it also inserts checks for the calling semantics, which will raise an exception if they are not adhered to. Utilities RequiresNoFinalisation Mixin class that can be used with StreamProcessor/StreamSink to automatically implement the finish method. It will call the done callback automatically for processors if it has not been called already. ProcessState Descriptor class which adds per-stream state to a stream-processing element. Takes an initialiser function as its only constructor argument. The initialiser is used to re-initialise the state before each stream is processed by the pipeline that contains the element. The state can be modified during stream processing to track per-stream state, but the changes are discarded once a new stream is processed.","title":"Stream Layer"},{"location":"stream/#stream-layer","text":"The stream layer provides classes for creating custom stream-processing pipelines. Each pipeline consists of: a source, which injects items into the pipeline for processing; processors, which take items and process them; and a sink, which consumes the items coming out of the pipeline. There are 3 base-classes representing sources, processors and sinks (respectively: StreamSource, StreamProcessor and StreamSink). These classes should be inherited to provide customised stream-processing functionality. The Pipeline class gathers the individual stream elements into a whole, connects each element to the previous and next elements, and ensures that the elements follow the pipeline's calling semantics (see below). Note that while the stream-processing elements are typed on the items they consume/produce, this is only for linting purposes and not enforced at run-time.","title":"Stream Layer"},{"location":"stream/#calling-semantics","text":"For sources and processors, the ability to pass items on to later elements in the processing pipeline is provided through 2 callback functions, then and done . then forwards an item to the next element of the pipeline, and done signals that no more items will be forwarded. This design was chosen over e.g. iterators as it ensures that when an error occurs, the relevant information which led to the error is retained on the call-stack. This is a useful aid for debugging. However, this does mean that the potential exists for the callbacks to called in an arbitrary order, when only certain permutations are sensible. The intended calling semantics of the 2 callbacks are: 0 or more calls to then to forward items, followed by exactly 1 call to done to signal termination. The pipeline class constructs these callbacks so that, if they are not called in the correct order, an error occurs. Specifically, calling then after done has been called will raise an error, as will not calling done at all. In practice, done is written to be idempotent, such that calling it multiple times is the same as calling it once.","title":"Calling semantics"},{"location":"stream/#streamsource","text":"Stream sources should implement the produce method to inject items into the pipeline. The method should call the then callback with each item to inject as its argument. Once all items have been produced, the source should then call the done callback.","title":"StreamSource"},{"location":"stream/#streamprocessor","text":"Stream processors should implement process_element method to receive items from the previous processing-element in the pipeline for processing. They should also implement the finish method to finalise processing after all items have been received. Both methods take the then and done callbacks as arguments, so processors can process items and forward the processed results as they arrive, or batch them and perform processing/forwarding in bulk. An optional start method is available, which can be overridden to perform any set-up required before the processor starts receiving items.","title":"StreamProcessor"},{"location":"stream/#streamsink","text":"Stream sinks should implement the consume_element method to consume items produced by the pipeline. They should also implement the finish method to perform any tidy-up once the pipeline has terminated. An optional start method is available, which can be overridden to perform any set-up required before the sink starts receiving items.","title":"StreamSink"},{"location":"stream/#pipeline","text":"The Pipeline class take an optional source, zero or more processors, and an optional sink as its constructor arguments. When the process method is called, the processing elements are connected together, the start methods for each processor and the sink are called, and the source is instructed to start producing items. Optionally, an iterable can be passed to the process method to replace the pipeline's own source, and/or a function can be passed to replace the pipeline's sink. When the pipeline is connecting the processing elements together, it also inserts checks for the calling semantics, which will raise an exception if they are not adhered to.","title":"Pipeline"},{"location":"stream/#utilities","text":"","title":"Utilities"},{"location":"stream/#requiresnofinalisation","text":"Mixin class that can be used with StreamProcessor/StreamSink to automatically implement the finish method. It will call the done callback automatically for processors if it has not been called already.","title":"RequiresNoFinalisation"},{"location":"stream/#processstate","text":"Descriptor class which adds per-stream state to a stream-processing element. Takes an initialiser function as its only constructor argument. The initialiser is used to re-initialise the state before each stream is processed by the pipeline that contains the element. The state can be modified during stream processing to track per-stream state, but the changes are discarded once a new stream is processed.","title":"ProcessState"},{"location":"usage/","text":"How to use wai-annotations from the command-line To convert a dataset using wai-annotations from the command-line, run the following command: wai-annotations convert [CONVERSION OPTIONS] \\ input-type [INPUT OPTIONS] \\ [ISP/XDC [ISP/XDC OPTIONS]]... \\ output-type [OUTPUT OPTIONS] (ISP=inline-stream-processor, XDC=cross-domain-conversion) For the available conversion options, see here . To list of available plugins in your environment, run: wai-annotations plugins For the available domains in your environment, run: wai-annotations domains The -h/--help option can be given at any point in a command-string to provide the options available at that point in the command. Examples of how to run wai-annotations can be found here . Logging By default, all logging messages are being output. To change that, you can use the WAIANN_LOG_LEVEL environment variable, using one of the following levels (the higher the number the fewer messages): 10: DEBUG 20: INFO 30: WARNING 40: ERROR","title":"Usage"},{"location":"usage/#how-to-use-wai-annotations-from-the-command-line","text":"To convert a dataset using wai-annotations from the command-line, run the following command: wai-annotations convert [CONVERSION OPTIONS] \\ input-type [INPUT OPTIONS] \\ [ISP/XDC [ISP/XDC OPTIONS]]... \\ output-type [OUTPUT OPTIONS] (ISP=inline-stream-processor, XDC=cross-domain-conversion) For the available conversion options, see here . To list of available plugins in your environment, run: wai-annotations plugins For the available domains in your environment, run: wai-annotations domains The -h/--help option can be given at any point in a command-string to provide the options available at that point in the command. Examples of how to run wai-annotations can be found here .","title":"How to use wai-annotations from the command-line"},{"location":"usage/#logging","text":"By default, all logging messages are being output. To change that, you can use the WAIANN_LOG_LEVEL environment variable, using one of the following levels (the higher the number the fewer messages): 10: DEBUG 20: INFO 30: WARNING 40: ERROR","title":"Logging"}]}